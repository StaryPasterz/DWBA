diff --git a/DW_main.py b/DW_main.py
index d3370e2..0f5aeee 100644
--- a/DW_main.py
+++ b/DW_main.py
@@ -41,6 +41,7 @@ import os
 import sys
 import glob
 import argparse
+import builtins
 from dataclasses import asdict
 
 from driver import (
@@ -69,6 +70,61 @@ from calibration import TongModel
 from output_utils import get_results_dir, get_output_path, get_json_path, find_result_files
 from logging_config import get_logger
 
+# -----------------------------------------------------------------------------
+# Console output safety (Windows code pages)
+# -----------------------------------------------------------------------------
+# Some terminals (e.g. cp1250/cp1252) cannot encode box-drawing symbols used by
+# this CLI, which may raise UnicodeEncodeError and abort batch runs.
+#
+# Strategy:
+# 1) Try UTF-8 reconfigure when available.
+# 2) Wrap module-local print() with graceful ASCII fallback on encoding errors.
+# -----------------------------------------------------------------------------
+if os.name == "nt":
+    if hasattr(sys.stdout, "reconfigure"):
+        try:
+            sys.stdout.reconfigure(encoding="utf-8")
+        except Exception:
+            pass
+    if hasattr(sys.stderr, "reconfigure"):
+        try:
+            sys.stderr.reconfigure(encoding="utf-8")
+        except Exception:
+            pass
+
+
+def _safe_print(*args, **kwargs) -> None:
+    """Print with fallback for non-UTF console encodings."""
+    try:
+        builtins.print(*args, **kwargs)
+        return
+    except UnicodeEncodeError:
+        pass
+
+    encoding = getattr(sys.stdout, "encoding", None) or "utf-8"
+
+    def _normalize(s: str) -> str:
+        # Replace decorative glyphs with ASCII-safe equivalents.
+        s = (
+            s.replace("Ôćĺ", "->")
+            .replace("Ôťô", "[OK]")
+            .replace("ÔťŚ", "[X]")
+            .replace("ÔÜá", "[!]")
+            .replace("ÔÇó", "-")
+            .replace("ÔĽö", "+").replace("ÔĽŚ", "+").replace("ÔĽÜ", "+").replace("ÔĽŁ", "+")
+            .replace("ÔĽÉ", "=").replace("ÔöÇ", "-")
+            .replace("Ôöé", "|")
+            .replace("Ôöî", "+").replace("ÔöÉ", "+").replace("Ôöö", "+").replace("Ôöś", "+")
+        )
+        return s.encode(encoding, errors="replace").decode(encoding, errors="replace")
+
+    safe_args = [_normalize(str(a)) for a in args]
+    builtins.print(*safe_args, **kwargs)
+
+
+# Module-local override: all print() calls in this file use safe output.
+print = _safe_print
+
 # Initialize logger for debug messages
 logger = get_logger(__name__)
 
@@ -674,8 +730,10 @@ def check_file_exists_warning(filename) -> bool:
     Warn the user if the output file already exists.
     Returns True if user wants to continue (append), False to abort.
     """
-    if os.path.exists(filename):
-        print(f"\n[WARNING] File '{filename}' already exists!")
+    # Check in results/ directory where files are actually saved
+    results_path = os.path.join("results", filename)
+    if os.path.exists(results_path):
+        print(f"\n[WARNING] File '{results_path}' already exists!")
         print("New results will be appended/merged into this file.")
         confirm = input("Continue? (y/N): ").strip().lower()
         if confirm != 'y':
@@ -877,7 +935,14 @@ def run_scan_excitation(run_name) -> None:
     n_points_max = params['grid'].get('n_points_max', 15000)
     min_pts_per_wl = params['grid'].get('min_points_per_wavelength', 15)
     
+    # Ionic charge for Coulomb asymptotic validity requirement
+    z_ion = core_params.Zc - 1.0  # For single-electron: He+ has Zc=2, z_ion=1
+    
     E_min_scan = float(np.min(energies))
+    E_max_scan = float(np.max(energies))
+    
+    # Use worst-case scan energy to size grid for effective runtime L usage.
+    L_max_effective = estimate_effective_projectile_lmax(E_max_scan, L_max_proj)
     
     print_subheader("Grid Configuration")
     
@@ -891,7 +956,7 @@ def run_scan_excitation(run_name) -> None:
     elif strategy == 'local':
         # LOCAL: Will recalculate per energy, but start with E_min for initial prep
         r_max_calc, n_points_calc = calculate_optimal_grid_params(
-            E_min_scan, L_max_proj, base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl
+            E_min_scan, L_max_effective, base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl, z_ion
         )
         print_info(f"Strategy: LOCAL (per-energy adaptive)")
         print_info(f"  Initial grid (E_min={E_min_scan:.1f} eV): r_max = {r_max_calc:.1f}, n_points = {n_points_calc}")
@@ -901,7 +966,7 @@ def run_scan_excitation(run_name) -> None:
         # GLOBAL: Calculate optimal grid for lowest energy, use for all
         k_min = k_from_E_eV(E_min_scan - dE_thr) if E_min_scan > dE_thr else k_from_E_eV(0.5)
         r_max_calc, n_points_calc = calculate_optimal_grid_params(
-            E_min_scan, L_max_proj, base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl
+            E_min_scan, L_max_effective, base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl, z_ion
         )
         print_info(f"Strategy: GLOBAL (single adaptive calculation)")
         print_info(f"  E_min = {E_min_scan:.1f} eV, k_min = {k_min:.2f} a.u.")
@@ -966,8 +1031,11 @@ def run_scan_excitation(run_name) -> None:
                 # LOCAL strategy: recalculate grid for each energy
                 current_prep = prep
                 if strategy == 'local':
+                    # Effective L for this energy (kept consistent with runtime logic)
+                    L_eff_local = estimate_effective_projectile_lmax(E, L_max_proj)
+                    
                     r_local, n_local = calculate_optimal_grid_params(
-                        E, L_max_proj, base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl
+                        E, L_eff_local, base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl, z_ion
                     )
                     
                     # For first iteration, ALWAYS recalculate to ensure correct size
@@ -1199,6 +1267,7 @@ def run_scan_ionization(run_name) -> None:
     
     # --- Adaptive r_max and n_points using unified function (v2.7+) ---
     E_min_scan = float(np.min(energies))
+    E_max_scan = float(np.max(energies))
     
     # Get grid parameters from params (supports user config)
     base_r_max = params['grid'].get('r_max', 200.0)
@@ -1207,10 +1276,14 @@ def run_scan_ionization(run_name) -> None:
     n_points_max = params['grid'].get('n_points_max', 10000)
     min_pts_per_wl = params['grid'].get('min_points_per_wavelength', 15)
     
+    # L_max_effective: estimate maximum L that will be used at runtime.
+    L_eff = estimate_effective_projectile_lmax(E_max_scan, L_max_proj)
+    z_ion = core_params.Zc - 1.0
+
     # Use unified grid calculation with wavelength scaling
     r_max_optimal, n_points_optimal = calculate_optimal_grid_params(
-        E_min_scan, L_max_proj, base_r_max, base_n_points, 
-        scale_factor, n_points_max, min_pts_per_wl
+        E_min_scan, L_eff, base_r_max, base_n_points, 
+        scale_factor, n_points_max, min_pts_per_wl, z_ion
     )
     
     print_info(f"Adaptive Grid: E_min={E_min_scan:.1f} eV -> r_max={r_max_optimal:.0f}, n_points={n_points_optimal}")
@@ -1386,7 +1459,8 @@ def run_dcs_visualization() -> None:
         # Get all energies from first key
         first_key = list(data.keys())[0]
         all_energies = [p["energy_eV"] for p in data[first_key] if p.get("theta_deg")]
-    except:
+    except (json.JSONDecodeError, KeyError, IndexError) as e:
+        logger.debug("Could not parse energy list from results: %s", e)
         all_energies = []
     
     if all_energies:
@@ -1403,7 +1477,8 @@ def run_dcs_visualization() -> None:
                 selected_energies = [float(x.strip()) for x in user_input.split(',')]
                 # Pass selected energies to plotter via environment variable
                 os.environ['DCS_ENERGIES'] = ','.join([str(e) for e in selected_energies])
-            except:
+            except ValueError as e:
+                logger.debug("Invalid energy input: %s", e)
                 print("Invalid input. Using all energies.")
                 if 'DCS_ENERGIES' in os.environ:
                     del os.environ['DCS_ENERGIES']
@@ -1520,14 +1595,48 @@ def main() -> None:
 # Helper Utilities
 # =============================================================================
 
+def estimate_effective_projectile_lmax(
+    E_eV: float,
+    L_max_projectile_base: int,
+    dynamic_scale: float = 8.0,
+    dynamic_offset: int = 5,
+    chi_f_buffer: int = 15,
+) -> int:
+    """
+    Estimate effective projectile L_max used by continuum caching/propagation.
+
+    Runtime uses a dynamic estimate L_dynamic ~= k*8+5 and final-channel
+    wave caching with an additional buffer (+15). This helper centralizes that
+    logic so grid sizing and runtime numerics stay consistent.
+    """
+    k_au = k_from_E_eV(E_eV)
+    L_dynamic = int(k_au * dynamic_scale) + dynamic_offset
+    return max(L_dynamic + chi_f_buffer, int(L_max_projectile_base) + chi_f_buffer)
+
+
+def resolve_grid_r_max_for_prep(base_r_max: float | str, fallback_auto: float = 200.0) -> float:
+    """
+    Resolve r_max to a numeric value for target preparation.
+
+    When config uses r_max='auto', target preparation still needs a concrete
+    value to build the grid. We use a stable fallback (200 a.u.) for this step.
+    """
+    if isinstance(base_r_max, str):
+        if base_r_max.strip().lower() == "auto":
+            return float(fallback_auto)
+        raise ValueError(f"Unsupported r_max string '{base_r_max}'. Use numeric value or 'auto'.")
+    return float(base_r_max)
+
+
 def calculate_optimal_grid_params(
     E_eV: float, 
     L_max_proj: int,
-    base_r_max: float,
+    base_r_max: float | str,
     base_n_points: int,
     scale_factor: float = 2.5,
     n_points_max: int = 15000,
-    min_points_per_wavelength: int = 15
+    min_points_per_wavelength: int = 15,
+    z_ion: float = 0.0
 ) -> tuple[float, int]:
     """
     Calculate optimal radial grid size based on energy and projectile L_max.
@@ -1535,16 +1644,24 @@ def calculate_optimal_grid_params(
     Strategy:
     1. Determine minimum r_max needed to contain the classical turning point
        for L_max_proj with a given safety margin.
-    2. Enforce the user's base_r_max as a minimum floor.
-    3. Scale n_points proportionally to maintain grid density.
-    4. (v2.7+) For high energies, ensure sufficient points per wavelength
+    2. For ionic targets (z_ion > 0), also ensure Coulomb asymptotic validity.
+    3. Enforce the user's base_r_max as a minimum floor.
+    4. Scale n_points proportionally to maintain grid density.
+    5. (v2.7+) For high energies, ensure sufficient points per wavelength
        to accurately sample oscillations.
     
     Parameters
     ----------
+    base_r_max : float | str
+        User-configured base radius. Accepts numeric value or "auto".
+        For "auto", the floor is determined by physics (`r_needed`) and
+        density scaling uses a reference radius of 200 a.u.
     min_points_per_wavelength : int
         Minimum grid points per wavelength at large r. Default 15.
         Set to 0 to disable wavelength-based scaling.
+    z_ion : float
+        Ionic charge of target (0 for neutral, 1 for He+, etc.).
+        Affects r_max requirement for Coulomb asymptotic validity.
     
     Returns
     -------
@@ -1553,13 +1670,18 @@ def calculate_optimal_grid_params(
     # Get wave number
     k_au = k_from_E_eV(E_eV)
     
-    # Compute physically required r_max from turning point logic
-    # r_max >= safety * (L+0.5)/k
-    r_needed = compute_required_r_max(k_au, L_max_proj, scale_factor)
+    # Compute physically required r_max from turning point AND Coulomb logic
+    r_needed = compute_required_r_max(k_au, L_max_proj, scale_factor, z_ion)
+    
+    # Handle 'auto' string for base_r_max.
+    # If 'auto', we use r_needed as the floor and scale density against a
+    # reference radius (200 a.u.) to avoid under-resolving very large grids.
+    is_auto_rmax = str(base_r_max).strip().lower() == 'auto'
+    base_r_val = 0.0 if is_auto_rmax else float(base_r_max)
     
     # We want at least the base config (e.g. 200 a.u.)
     # but extend if physics demands it (low energy / high L)
-    r_max_optimal = max(base_r_max, r_needed)
+    r_max_optimal = max(base_r_val, r_needed)
     
     # --- Wavelength-based n_points scaling for high energies ---
     # For exponential grid: dr(r) Ôëł r * ln(r_max/r_min) / n_points
@@ -1600,10 +1722,13 @@ def calculate_optimal_grid_params(
     else:
         n_points_optimal = base_n_points
     
-    # Check if we are extending r_max beyond base
-    if r_max_optimal > base_r_max:
+    # Check if we are extending r_max beyond base/reference.
+    # For 'auto', use 200 a.u. as a practical density reference so n_points
+    # still scales with large r_max values instead of staying fixed.
+    density_ref_r = 200.0 if is_auto_rmax else base_r_val
+    if density_ref_r > 0 and r_max_optimal > density_ref_r:
         # Scale points to keep density constant
-        density = base_n_points / base_r_max
+        density = base_n_points / density_ref_r
         n_density_req = int(density * r_max_optimal)
         n_points_optimal = max(n_points_optimal, n_density_req)
     
@@ -1679,18 +1804,28 @@ def run_pilot_calibration(
     pilot_L_proj_cfg = params['excitation'].get('pilot_L_max_projectile', 'auto')
     pilot_L_int_cfg = params['excitation'].get('pilot_L_max_integrals', 'auto')
     
+    # Convert pilot energy to wave number: k = sqrt(2*E/E_h) where E_h = 27.2114 eV (1 Hartree)
+    # This is needed for both dynamic L_max and grid scaling calculations
+    k_pilot = np.sqrt(2 * pilot_E / 27.2114)  # k in a.u. (bohr^-1)
+    
     # Calculate dynamic L_max if needed
     if pilot_L_proj_cfg == 'auto' or pilot_L_int_cfg == 'auto':
-        k_pilot = np.sqrt(2 * pilot_E / 27.2114)  # k in a.u.
         r_max_grid = params['grid']['r_max']
-        pilot_L_proj_dynamic = int(k_pilot * r_max_grid * 0.6)
+        # Handle 'auto' string - use default 200 a.u. for L_max estimation
+        if str(r_max_grid).lower() == 'auto':
+            r_max_grid = 200.0  # Default for pilot L_max calculation
+        # Estimate L_max from classical turning point: L ~ k*r * scale_factor
+        # 0.6 = conservative scale factor to ensure turning point is well within grid
+        pilot_L_proj_dynamic = int(k_pilot * float(r_max_grid) * 0.6)
         
         if pilot_L_proj_cfg == 'auto':
+            # Cap at 150 to prevent excessive computation time (L^2 scaling)
             pilot_L_proj = max(spec.L_max_projectile, min(pilot_L_proj_dynamic, 150))
         else:
             pilot_L_proj = int(pilot_L_proj_cfg)
         
         if pilot_L_int_cfg == 'auto':
+            # L_max for multipole expansion: typically L_proj/4, capped at 25 for efficiency
             pilot_L_int = max(spec.L_max_integrals, min(25, pilot_L_proj // 4))
         else:
             pilot_L_int = int(pilot_L_int_cfg)
@@ -1707,17 +1842,21 @@ def run_pilot_calibration(
     tong_model = TongModel(threshold_eV, epsilon_exc_au, transition_class=transition_class)
     alpha = 1.0
     res_pilot = None
-    
     try:
+        # Calculate z_ion and L_max_effective for proper Coulomb r_max scaling
+        z_ion = core_params.Zc - 1.0
+        L_max_eff_pilot = estimate_effective_projectile_lmax(pilot_E, pilot_L_proj)
+        
         # Calculate adaptive grid for pilot energy
         pilot_r_max, pilot_n_points = calculate_optimal_grid_params(
             pilot_E, 
-            pilot_L_proj,
+            L_max_eff_pilot,
             base_r_max=params['grid']['r_max'],
             base_n_points=params['grid']['n_points'],
             scale_factor=params['grid'].get('r_max_scale_factor', 2.5),
             n_points_max=params['grid'].get('n_points_max', 15000),
-            min_points_per_wavelength=params['grid'].get('min_points_per_wavelength', 15)
+            min_points_per_wavelength=params['grid'].get('min_points_per_wavelength', 15),
+            z_ion=z_ion
         )
         
         # Log pilot calculation start with grid info
@@ -1853,24 +1992,27 @@ def run_from_config(config_path: str, verbose: bool = False) -> None:
         min_pts_per_wl = params['grid'].get('min_points_per_wavelength', 15)
         
         # Determine parameters for initial target prep
+        E_ref = min(energy_grid)
+        E_max = max(energy_grid)
+        L_eff = estimate_effective_projectile_lmax(E_max, params['excitation']['L_max_projectile'])
+        z_ion = core_params.Zc - 1.0
+
         if strategy == 'manual':
             r_max_calc, n_points_calc = base_r_max, base_n_points
             logger.info("Grid Strategy   | MANUAL (r_max=%.1f a.u., n_points=%d)", 
                        r_max_calc, n_points_calc)
         elif strategy == 'local':
-            E_ref = min(energy_grid)
             r_max_calc, n_points_calc = calculate_optimal_grid_params(
-                E_ref, params['excitation']['L_max_projectile'],
-                base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl
+                E_ref, L_eff,
+                base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl, z_ion
             )
             logger.info("Grid Strategy   | LOCAL (per-energy adaptive)")
             logger.info("Grid Initial    | E_min=%.1f eV: r_max=%.1f a.u., n_points=%d", 
                        E_ref, r_max_calc, n_points_calc)
         else:  # 'global' (default)
-            E_ref = min(energy_grid)
             r_max_calc, n_points_calc = calculate_optimal_grid_params(
-                E_ref, params['excitation']['L_max_projectile'],
-                base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl
+                E_ref, L_eff,
+                base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl, z_ion
             )
             logger.info("Grid Strategy   | GLOBAL (E_min=%.1f eV: r_max=%.1f a.u., n_points=%d)", 
                        E_ref, r_max_calc, n_points_calc)
@@ -1970,9 +2112,14 @@ def run_from_config(config_path: str, verbose: bool = False) -> None:
                     # LOCAL ADAPTIVE: Re-calculate target if needed
                     current_prep = prep
                     if strategy == 'local':
+                        # Use same L_eff logic as interactive mode
+                        L_eff_local = estimate_effective_projectile_lmax(
+                            E, params['excitation']['L_max_projectile']
+                        )
+                        
                         r_local, n_local = calculate_optimal_grid_params(
-                            E, params['excitation']['L_max_projectile'],
-                            base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl
+                            E, L_eff_local,
+                            base_r_max, base_n_points, scale_factor, n_points_max, min_pts_per_wl, z_ion
                         )
                         
                         # For first iteration, ALWAYS recalculate to ensure correct size
@@ -2079,16 +2226,22 @@ def run_from_config(config_path: str, verbose: bool = False) -> None:
             L_max_projectile=params['ionization']['L_max_projectile']
         )
         
-        # Get ionization threshold (reuse grid from prepare_target pattern)
-        from bound_states import solve_bound_states
-        from grid import make_r_grid
-        from potential_core import V_core_on_grid
+        # Get ionization threshold and reusable target preparation
         from driver import prepare_target
         
         # Pre-calculate target properties
         print("  Pre-calculating static target properties...", end=" ", flush=True)
         use_pol = (config.physics_model == "polarization")
         
+        # Grid strategy (aligned with interactive ionization/excitation flows)
+        strategy = params['grid'].get('strategy', 'global').lower()
+        base_r_max = params['grid'].get('r_max', 200.0)
+        base_n_points = int(params['grid'].get('n_points', 4000))
+        scale_factor = params['grid'].get('r_max_scale_factor', 2.5)
+        n_points_max = params['grid'].get('n_points_max', 10000)
+        min_pts_per_wl = params['grid'].get('min_points_per_wavelength', 15)
+        z_ion = core_params.Zc - 1.0
+        
         # Create a dummy excitation spec for prepare_target
         tmp_chan = ExcitationChannelSpec(
             l_i=li, l_f=li+1 if li < 3 else li, 
@@ -2097,17 +2250,17 @@ def run_from_config(config_path: str, verbose: bool = False) -> None:
             L_target_i=li, L_target_f=li+1 if li < 3 else li
         )
         
-        prep = prepare_target(
+        # First prep for threshold extraction only (supports r_max='auto').
+        prep_threshold = prepare_target(
             chan=tmp_chan,
             core_params=core_params,
             use_polarization=use_pol,
-            r_max=params['grid']['r_max'],
-            n_points=params['grid']['n_points']
+            r_max=resolve_grid_r_max_for_prep(base_r_max),
+            n_points=base_n_points
         )
-        print("done")
         
         # Get threshold from prep
-        E_bind_au = prep.orb_i.energy_au
+        E_bind_au = prep_threshold.orb_i.energy_au
         threshold_eV = abs(E_bind_au) / ev_to_au(1.0)
         
         print(f"  Ionization potential: {threshold_eV:.2f} eV")
@@ -2120,6 +2273,55 @@ def run_from_config(config_path: str, verbose: bool = False) -> None:
         
         print(f"  Grid: {len(energies)} points above threshold")
         
+        E_min_scan = float(np.min(energies))
+        E_max_scan = float(np.max(energies))
+        L_eff_scan = estimate_effective_projectile_lmax(
+            E_max_scan, params['ionization']['L_max_projectile']
+        )
+        
+        if strategy == "manual":
+            if isinstance(base_r_max, str):
+                print_error("grid.r_max='auto' is not valid with strategy='manual'.")
+                return
+            r_max_calc = float(base_r_max)
+            n_points_calc = base_n_points
+            logger.info(
+                "Grid Strategy   | MANUAL (r_max=%.1f a.u., n_points=%d)",
+                r_max_calc, n_points_calc
+            )
+        else:
+            r_max_calc, n_points_calc = calculate_optimal_grid_params(
+                E_min_scan,
+                L_eff_scan,
+                base_r_max,
+                base_n_points,
+                scale_factor,
+                n_points_max,
+                min_pts_per_wl,
+                z_ion,
+            )
+            if strategy == "local":
+                logger.info("Grid Strategy   | LOCAL (per-energy adaptive)")
+                logger.info(
+                    "Grid Initial    | E_min=%.1f eV: r_max=%.1f a.u., n_points=%d",
+                    E_min_scan, r_max_calc, n_points_calc
+                )
+            else:
+                logger.info(
+                    "Grid Strategy   | GLOBAL (E_min=%.1f eV: r_max=%.1f a.u., n_points=%d)",
+                    E_min_scan, r_max_calc, n_points_calc
+                )
+        
+        # Main prep for calculations (manual/global fixed baseline).
+        prep = prepare_target(
+            chan=tmp_chan,
+            core_params=core_params,
+            use_polarization=use_pol,
+            r_max=r_max_calc,
+            n_points=n_points_calc
+        )
+        print("done")
+        
         # Build result key (same format as interactive)
         key = f"Ionization_Z{core_params.Zc:.0f}_{config.target.atom}_n{ni}l{li}"
         filename = f"results_{run_name}_ion.json"
@@ -2132,17 +2334,71 @@ def run_from_config(config_path: str, verbose: bool = False) -> None:
         
         results = []
         try:
-            for E in energies:
+            for i_e, E in enumerate(energies):
                 try:
+                    current_prep = prep
+                    r_eval = r_max_calc
+                    n_eval = n_points_calc
+                    
+                    if strategy == "local":
+                        L_eff_local = estimate_effective_projectile_lmax(
+                            E, params['ionization']['L_max_projectile']
+                        )
+                        r_local, n_local = calculate_optimal_grid_params(
+                            E,
+                            L_eff_local,
+                            base_r_max,
+                            base_n_points,
+                            scale_factor,
+                            n_points_max,
+                            min_pts_per_wl,
+                            z_ion,
+                        )
+                        if i_e == 0:
+                            current_prep = prepare_target(
+                                chan=tmp_chan,
+                                core_params=core_params,
+                                use_polarization=use_pol,
+                                r_max=r_local,
+                                n_points=n_local
+                            )
+                            logger.info(
+                                "Local Adaptive  | E=%.1f eV: r_max=%.1f a.u., n_points=%d (initial)",
+                                E, r_local, n_local
+                            )
+                            prep = current_prep
+                        else:
+                            current_r_max = prep.grid.r[-1]
+                            current_n_pts = len(prep.grid.r)
+                            params_changed = (
+                                abs(r_local - current_r_max) > 0.1 or
+                                abs(n_local - current_n_pts) > 1
+                            )
+                            if params_changed:
+                                current_prep = prepare_target(
+                                    chan=tmp_chan,
+                                    core_params=core_params,
+                                    use_polarization=use_pol,
+                                    r_max=r_local,
+                                    n_points=n_local
+                                )
+                                logger.info(
+                                    "Local Adaptive  | E=%.1f eV: r_max=%.1f a.u., n_points=%d",
+                                    E, r_local, n_local
+                                )
+                                prep = current_prep
+                        r_eval = r_local
+                        n_eval = n_local
+                    
                     ion_res = compute_ionization_cs(
                         E, ion_spec, core_params,
-                        r_max=params['grid']['r_max'],
-                        n_points=params['grid']['n_points'],
+                        r_max=r_eval,
+                        n_points=n_eval,
                         use_polarization=use_pol,
                         n_energy_steps=params['ionization']['n_energy_steps'],
-                        _precalc_grid=prep.grid,
-                        _precalc_V_core=prep.V_core,
-                        _precalc_orb_i=prep.orb_i
+                        _precalc_grid=current_prep.grid,
+                        _precalc_V_core=current_prep.V_core,
+                        _precalc_orb_i=current_prep.orb_i
                     )
                     
                     print_result(E, ion_res.sigma_total_cm2)
diff --git a/driver.py b/driver.py
index d49e4bb..37b84d9 100644
--- a/driver.py
+++ b/driver.py
@@ -116,6 +116,13 @@ OSCILLATORY_CONFIG = {
 # =============================================================================
 _SCAN_LOGGED = False  # True after first energy point logs hardware info
 
+
+def _is_hotpath_debug_enabled() -> bool:
+    """
+    Enable very verbose per-(l_i,l_f) debug only on explicit request.
+    """
+    return os.environ.get("DWBA_HOTPATH_DEBUG", "").strip().lower() in {"1", "true", "yes", "on"}
+
 def reset_scan_logging():
     """Reset scan-level logging flags. Call at start of new energy scan."""
     global _SCAN_LOGGED
@@ -322,7 +329,8 @@ def _worker_partial_wave(
     # Solve chi_i
     try:
         chi_i = solve_continuum_wave(grid, U_i, l_i, E_incident_eV, z_ion) 
-    except:
+    except Exception as e:
+        logger.debug("chi_i solve failed for l_i=%d: %s", l_i, e)
         chi_i = None
 
     if chi_i is None:
@@ -339,8 +347,9 @@ def _worker_partial_wave(
             
         try:
             chi_f = solve_continuum_wave(grid, U_f, l_f, E_final_eV, z_ion)
-        except:
-             chi_f = None
+        except Exception as e:
+            logger.debug("chi_f solve failed for l_f=%d: %s", l_f, e)
+            chi_f = None
         if chi_f is None: continue
 
         # Integrals
@@ -404,7 +413,8 @@ def _worker_solve_wave(
             solver=solver
         )
         return l, chi
-    except:
+    except Exception as e:
+        logger.debug("Worker wave solve failed for l=%d: %s", l, e)
         return l, None
 
 def precompute_continuum_waves(
@@ -703,6 +713,15 @@ def compute_total_excitation_cs(
             
             lf_min = 0
             lf_max = max(L_max_proj, l_i + chan.L_max_integrals)
+            li_iter_start = time.perf_counter()
+            last_li_heartbeat = li_iter_start
+            heartbeat_every_s = float(OSCILLATORY_CONFIG.get("gpu_li_heartbeat_s", 120.0))
+            slow_pair_warn_s = float(OSCILLATORY_CONFIG.get("gpu_pair_warn_s", 20.0))
+            lf_parity_total = sum(
+                1 for lf in range(lf_min, lf_max + 1)
+                if (l_i + lf) % 2 == target_parity_change
+            )
+            lf_done = 0
             
             # Local amplitude for this l_i
             li_amplitudes = {}
@@ -716,6 +735,15 @@ def compute_total_excitation_cs(
             any_valid_lf = False
             for l_f in range(lf_min, lf_max + 1):
                 if (l_i + l_f) % 2 != target_parity_change: continue
+                lf_done += 1
+
+                now = time.perf_counter()
+                if heartbeat_every_s > 0 and (now - last_li_heartbeat) >= heartbeat_every_s:
+                    logger.info(
+                        "Summing heartbeat: l_i=%d/%d, l_f=%d/%d (current l_f=%d, elapsed %.1fs)",
+                        l_i, L_max_proj, lf_done, max(1, lf_parity_total), l_f, now - li_iter_start
+                    )
+                    last_li_heartbeat = now
                 
                 # Use Cache for chi_f
                 if l_f in chi_f_cache:
@@ -723,27 +751,30 @@ def compute_total_excitation_cs(
                 else:
                     try:
                         chi_f = solve_continuum_wave(grid, U_f, l_f, E_final_eV, z_ion)
-                    except:
+                    except Exception as e:
+                        logger.debug("GPU path chi_f solve failed for l_f=%d: %s", l_f, e)
                         chi_f = None
                         
                 if chi_f is None: continue
                 
                 any_valid_lf = True
                 
-                # --- DEBUG: Log array sizes before GPU integrals ---
-                logger.debug(
-                    "GPU INTEGRALS DEBUG | l_i=%d, l_f=%d | "
-                    "grid.r=%d, V_core=%d, U_i=%d, U_f=%d | "
-                    "orb_i.u=%d, orb_f.u=%d | "
-                    "chi_i.chi=%d (idx_m=%d), chi_f.chi=%d (idx_m=%d)",
-                    l_i, l_f,
-                    len(grid.r), len(V_core), len(U_i.U_of_r), len(U_f.U_of_r),
-                    len(orb_i.u_of_r), len(orb_f.u_of_r),
-                    len(chi_i.chi_of_r), chi_i.idx_match,
-                    len(chi_f.chi_of_r), chi_f.idx_match
-                )
+                # Very verbose hot-path debug can significantly slow long runs.
+                if _is_hotpath_debug_enabled():
+                    logger.debug(
+                        "GPU INTEGRALS DEBUG | l_i=%d, l_f=%d | "
+                        "grid.r=%d, V_core=%d, U_i=%d, U_f=%d | "
+                        "orb_i.u=%d, orb_f.u=%d | "
+                        "chi_i.chi=%d (idx_m=%d), chi_f.chi=%d (idx_m=%d)",
+                        l_i, l_f,
+                        len(grid.r), len(V_core), len(U_i.U_of_r), len(U_f.U_of_r),
+                        len(orb_i.u_of_r), len(orb_f.u_of_r),
+                        len(chi_i.chi_of_r), chi_i.idx_match,
+                        len(chi_f.chi_of_r), chi_f.idx_match
+                    )
                 
                 # --- GPU INTEGRALS ---
+                t_pair = time.perf_counter()
                 integrals = radial_ME_all_L_gpu(
                     grid, V_core, U_i.U_of_r, orb_i, orb_f, chi_i, chi_f, chan.L_max_integrals,
                     use_oscillatory_quadrature=True,
@@ -758,6 +789,15 @@ def compute_total_excitation_cs(
                     gpu_cache=gpu_cache,  # Phase 3: pass energy-level cache
                     U_f_array=U_f.U_of_r  # Bug #2 fix: check both potentials
                 )
+                pair_elapsed = time.perf_counter() - t_pair
+                if slow_pair_warn_s > 0 and pair_elapsed >= slow_pair_warn_s:
+                    logger.warning(
+                        "Slow GPU pair: l_i=%d, l_f=%d took %.1fs (idx_i=%d, idx_f=%d, L_int=%d)",
+                        l_i, l_f, pair_elapsed,
+                        getattr(chi_i, "idx_match", -1),
+                        getattr(chi_f, "idx_match", -1),
+                        chan.L_max_integrals
+                    )
                 
                 # Distribute (CPU - fast)
                 for Mi in range(-Li, Li+1):
diff --git a/dwba_matrix_elements.py b/dwba_matrix_elements.py
index 7f794c8..46017ec 100644
--- a/dwba_matrix_elements.py
+++ b/dwba_matrix_elements.py
@@ -35,8 +35,9 @@ All inputs/outputs in Hartree atomic units (aÔéÇ, Ha).
 
 
 from __future__ import annotations
+import logging
+import os
 import numpy as np
-import gc
 from dataclasses import dataclass
 from typing import Dict, List, Union, Optional
 
@@ -77,6 +78,41 @@ from logging_config import get_logger
 # Initialize module logger
 logger = get_logger(__name__)
 
+
+def _is_hotpath_verbose_debug_enabled() -> bool:
+    """
+    Return True when verbose per-call GPU hot-path debug is explicitly enabled.
+
+    Set environment variable `DWBA_HOTPATH_DEBUG=1` to force detailed logging.
+    """
+    return os.environ.get("DWBA_HOTPATH_DEBUG", "").strip().lower() in {"1", "true", "yes", "on"}
+
+
+def _should_sampled_hotpath_debug(
+    key: str,
+    every: int = 200,
+    initial: int = 3
+) -> bool:
+    """
+    Sample hot-path DEBUG logs to avoid I/O bottlenecks in tight GPU loops.
+    """
+    if not logger.isEnabledFor(logging.DEBUG):
+        return False
+    if _is_hotpath_verbose_debug_enabled():
+        return True
+
+    counters = getattr(_should_sampled_hotpath_debug, "_counters", None)
+    if counters is None:
+        counters = {}
+        _should_sampled_hotpath_debug._counters = counters
+
+    count = counters.get(key, 0) + 1
+    counters[key] = count
+
+    if count <= initial:
+        return True
+    return (count % max(1, int(every))) == 0
+
 def check_cupy_runtime() -> bool:
     """
     Verifies if CuPy can actually run on this system.
@@ -123,12 +159,24 @@ class GPUCache:
         Base kernel matrix 1/r_> (idx_limit ├Ś idx_limit).
     log_ratio : cupy.ndarray or None
         Base ratio log(r_</r_>) matrix.
+    ratio : cupy.ndarray or None
+        Cached exp(log_ratio) for fast recursive L-updates.
+    filon_inv_gtr : cupy.ndarray or None
+        Extended Filon kernel base 1/r_> (idx_limit ├Ś N_grid).
+    filon_log_ratio : cupy.ndarray or None
+        Extended Filon ratio log(r_</r_>) matrix (idx_limit ├Ś N_grid).
+    filon_ratio : cupy.ndarray or None
+        Cached exp(filon_log_ratio) for full Filon recursive L-updates.
+    kernel_cc_pre : dict or None
+        Precomputed Filon exchange kernel-at-CC base tensors.
     filon_params : dict or None
         Precomputed Filon quadrature parameters.
     idx_limit : int
         Grid index limit used for cached matrices.
     chi_cache : dict
         Cache of continuum waves: (channel, l) -> cp.ndarray.
+    array_cache : dict
+        Cache of static 1D arrays transferred to GPU (potentials/bound states).
     chi_lru : list
         LRU tracking for chi_cache eviction.
     max_chi_cached : int
@@ -138,17 +186,29 @@ class GPUCache:
     w_gpu: 'cp.ndarray'
     inv_gtr: Optional['cp.ndarray'] = None
     log_ratio: Optional['cp.ndarray'] = None
+    ratio: Optional['cp.ndarray'] = None
+    filon_inv_gtr: Optional['cp.ndarray'] = None
+    filon_log_ratio: Optional['cp.ndarray'] = None
+    filon_ratio: Optional['cp.ndarray'] = None
+    filon_idx_limit: int = -1
+    kernel_cc_pre: Optional[dict] = None
+    kernel_cc_key: Optional[tuple] = None
     filon_params: Optional[dict] = None
     idx_limit: int = -1
     # Phase 4: Continuum wave cache with LRU
     chi_cache: Optional[Dict] = None
+    array_cache: Optional[Dict] = None
     chi_lru: Optional[List] = None
     max_chi_cached: int = 20
+    v_diff_cache: Optional['cp.ndarray'] = None
+    v_diff_key: Optional[tuple] = None
     
     def __post_init__(self) -> None:
         """Initialize mutable fields after dataclass creation."""
         if self.chi_cache is None:
             self.chi_cache = {}
+        if self.array_cache is None:
+            self.array_cache = {}
         if self.chi_lru is None:
             self.chi_lru = []
     
@@ -224,15 +284,81 @@ class GPUCache:
         self.chi_lru.append(key)
         
         return chi_gpu
+
+    def get_static_array(
+        self,
+        arr: np.ndarray,
+        tag: str,
+        n_grid: Optional[int] = None
+    ) -> 'cp.ndarray':
+        """
+        Get a static 1D numpy array on GPU with pointer-based caching.
+
+        This avoids repeated cp.asarray transfers for data that stays constant
+        over many (l_i, l_f) matrix-element calls within one energy point.
+        """
+        arr_np = np.asarray(arr)
+        if arr_np.ndim != 1:
+            raise ValueError(f"Expected 1D array for tag={tag}, got shape={arr_np.shape}")
+
+        target_n = arr_np.shape[0] if n_grid is None else int(n_grid)
+        if arr_np.shape[0] < target_n:
+            raise ValueError(
+                f"Array '{tag}' has {arr_np.shape[0]} pts but requested {target_n}"
+            )
+
+        ptr = int(arr_np.__array_interface__["data"][0])
+        key = (tag, ptr, target_n)
+        cached = self.array_cache.get(key)
+        if cached is not None and len(cached) == target_n:
+            return cached
+
+        arr_view = arr_np[:target_n] if arr_np.shape[0] != target_n else arr_np
+        arr_gpu = cp.asarray(arr_view)
+
+        # Keep cache bounded to avoid unbounded growth if many unique arrays appear.
+        if len(self.array_cache) >= 64:
+            self.array_cache.clear()
+        self.array_cache[key] = arr_gpu
+        return arr_gpu
+
+    def get_v_diff(
+        self,
+        V_core_array: np.ndarray,
+        U_i_array: np.ndarray,
+        n_grid: int
+    ) -> 'cp.ndarray':
+        """
+        Get cached V_core-U_i on GPU for the current grid size.
+        """
+        v_np = np.asarray(V_core_array)
+        u_np = np.asarray(U_i_array)
+        if v_np.shape[0] < n_grid or u_np.shape[0] < n_grid:
+            raise ValueError(
+                f"Cannot build V_diff: V_core={v_np.shape[0]}, U_i={u_np.shape[0]}, n_grid={n_grid}"
+            )
+
+        key = (
+            int(v_np.__array_interface__["data"][0]),
+            int(u_np.__array_interface__["data"][0]),
+            int(n_grid),
+        )
+        if self.v_diff_cache is not None and self.v_diff_key == key and len(self.v_diff_cache) == n_grid:
+            return self.v_diff_cache
+
+        self.v_diff_cache = cp.asarray(v_np[:n_grid] - u_np[:n_grid])
+        self.v_diff_key = key
+        return self.v_diff_cache
     
     def build_kernel_matrix(self, idx_limit: int) -> None:
         """Build base kernel matrices for given idx_limit."""
-        if self.inv_gtr is not None and self.idx_limit == idx_limit:
+        if self.inv_gtr is not None and self.idx_limit >= idx_limit:
             return
         
         # Clear old matrices
         self.inv_gtr = None
         self.log_ratio = None
+        self.ratio = None
         
         r_sub = self.r_gpu[:idx_limit]
         r_col = r_sub[:, None]
@@ -245,23 +371,83 @@ class GPUCache:
         
         del r_col, r_row, ratio
         self.idx_limit = idx_limit
+
+    def get_kernel_ratio(self, idx_limit: int) -> Optional['cp.ndarray']:
+        """
+        Return cached exp(log_ratio) slice for requested idx_limit.
+        """
+        if self.log_ratio is None or self.idx_limit < idx_limit:
+            return None
+        if self.ratio is None or len(self.ratio) < idx_limit:
+            self.ratio = cp.exp(self.log_ratio)
+        return self.ratio[:idx_limit, :idx_limit]
+
+    def build_filon_kernel_matrix(self, idx_limit: int) -> None:
+        """
+        Build extended Filon base matrices for given idx_limit.
+
+        Shape is (idx_limit ├Ś N_grid) and can be reused across many (l_i, l_f)
+        calls at the same energy/grid.
+        """
+        if self.filon_inv_gtr is not None and self.filon_idx_limit >= idx_limit:
+            return
+
+        self.filon_inv_gtr = None
+        self.filon_log_ratio = None
+        self.filon_ratio = None
+
+        r_col = self.r_gpu[:idx_limit, None]
+        r_row_full = self.r_gpu[None, :]
+
+        self.filon_inv_gtr = 1.0 / cp.maximum(r_col, r_row_full + 1e-30)
+        ratio = cp.minimum(r_col, r_row_full) * self.filon_inv_gtr
+        ratio = cp.minimum(ratio, 1.0 - 1e-12)
+        self.filon_log_ratio = cp.log(ratio + 1e-30)
+
+        del r_col, r_row_full, ratio
+        self.filon_idx_limit = idx_limit
+
+    def get_filon_ratio(self, idx_limit: int) -> Optional['cp.ndarray']:
+        """
+        Return cached exp(filon_log_ratio) slice for requested idx_limit rows.
+        """
+        if self.filon_log_ratio is None or self.filon_idx_limit < idx_limit:
+            return None
+        if self.filon_ratio is None or self.filon_ratio.shape[0] < idx_limit:
+            self.filon_ratio = cp.exp(self.filon_log_ratio)
+        return self.filon_ratio[:idx_limit, :]
     
     def clear(self) -> None:
         """Clear all cached data (call at end of energy point)."""
         self.inv_gtr = None
         self.log_ratio = None
+        self.ratio = None
+        self.filon_inv_gtr = None
+        self.filon_log_ratio = None
+        self.filon_ratio = None
+        self.filon_idx_limit = -1
+        self.kernel_cc_pre = None
+        self.kernel_cc_key = None
         self.filon_params = None
         self.idx_limit = -1
+        self.v_diff_cache = None
+        self.v_diff_key = None
         # Phase 4: Clear chi cache
         if self.chi_cache:
             self.chi_cache.clear()
+        if self.array_cache:
+            self.array_cache.clear()
         if self.chi_lru:
             self.chi_lru.clear()
         if HAS_CUPY:
             cp.get_default_memory_pool().free_all_blocks()
 
 
-def _compute_optimal_block_size(n_grid: int, gpu_memory_threshold: float = 0.8) -> int:
+def _compute_optimal_block_size(
+    n_grid: int,
+    gpu_memory_threshold: float = 0.8,
+    effective_free_mem: Optional[int] = None
+) -> int:
     """
     Compute optimal block size based on available GPU memory.
     
@@ -273,7 +459,9 @@ def _compute_optimal_block_size(n_grid: int, gpu_memory_threshold: float = 0.8)
     n_grid : int
         Number of grid points (rows in block matrix).
     gpu_memory_threshold : float
-        Fraction of free GPU memory to use (default 0.7).
+        Fraction of free GPU memory to use (default 0.8).
+    effective_free_mem : int, optional
+        Effective free bytes (device-free + reusable pool) when available.
         
     Returns
     -------
@@ -284,8 +472,9 @@ def _compute_optimal_block_size(n_grid: int, gpu_memory_threshold: float = 0.8)
         return 2048
     
     try:
-        free_mem, total_mem = cp.cuda.Device().mem_info
-        usable_mem = free_mem * gpu_memory_threshold
+        free_mem, _total_mem = cp.cuda.Device().mem_info
+        avail_mem = int(effective_free_mem) if effective_free_mem is not None else int(free_mem)
+        usable_mem = avail_mem * gpu_memory_threshold
         
         # Each block: (n_grid, block_size) matrix ├Ś 8 bytes ├Ś 4 matrices
         bytes_per_column = n_grid * 8 * 4
@@ -295,7 +484,13 @@ def _compute_optimal_block_size(n_grid: int, gpu_memory_threshold: float = 0.8)
         block = max(512, min(max_block, 16384))
         block = (block // 512) * 512
         
-        logger.debug("GPU auto-tune: free=%.1f GB, computed block_size=%d", free_mem / 1e9, block)
+        if _should_sampled_hotpath_debug("gpu_auto_tune", every=250, initial=2):
+            logger.debug(
+                "GPU auto-tune: free=%.1f GB%s, computed block_size=%d",
+                free_mem / 1e9,
+                "" if effective_free_mem is None else f", effective={avail_mem / 1e9:.1f} GB",
+                block
+            )
         return block
     except Exception as e:
         logger.debug("GPU memory query failed: %s. Using default block size.", e)
@@ -628,12 +823,6 @@ def radial_ME_all_L(
     # This helps diagnose issues with high partial waves.
     # ==========================================================================
     
-    if use_oscillatory_quadrature:
-        k_total = k_i + k_f
-        max_phase, is_ok, prob_idx = check_phase_sampling(r[:idx_limit], k_total)
-        if not is_ok and idx_limit > 10:
-            log_phase_diagnostic(r[:idx_limit], k_i, k_f, l_i, l_f)
-    
     # Get phase shifts for analytical tail (if available)
     delta_i = cont_i.phase_shift if hasattr(cont_i, 'phase_shift') else 0.0
     delta_f = cont_f.phase_shift if hasattr(cont_f, 'phase_shift') else 0.0
@@ -641,9 +830,20 @@ def radial_ME_all_L(
     # Get Coulomb parameters for ionic targets
     eta_i = cont_i.eta if hasattr(cont_i, 'eta') else 0.0
     eta_f = cont_f.eta if hasattr(cont_f, 'eta') else 0.0
+    eta_total = eta_i + eta_f
     sigma_i = cont_i.sigma_l if hasattr(cont_i, 'sigma_l') else 0.0
     sigma_f = cont_f.sigma_l if hasattr(cont_f, 'sigma_l') else 0.0
     
+    k_total = k_i + k_f
+    if use_oscillatory_quadrature:
+        max_phase, is_ok, prob_idx = check_phase_sampling(
+            r[:idx_limit], k_total, eta_total=eta_total
+        )
+        if not is_ok and idx_limit > 10:
+            log_phase_diagnostic(
+                r[:idx_limit], k_i, k_f, l_i, l_f, eta_total=eta_total
+            )
+    
     r_match = r[idx_limit - 1] if idx_limit > 0 else r[-1]
     
     for L in range(L_max + 1):
@@ -666,7 +866,8 @@ def radial_ME_all_L(
                 I_dir = oscillatory_kernel_integral_2d(
                     rho1_dir_uw, rho2_dir_uw, kernel_L, r, k_i, k_f, idx_limit,
                     method="filon", w_grid=w,
-                    n_nodes=CC_nodes, phase_increment=phase_increment
+                    n_nodes=CC_nodes, phase_increment=phase_increment,
+                    eta_total=eta_total
                 )
             
             elif oscillatory_method == "full_split":
@@ -691,7 +892,8 @@ def radial_ME_all_L(
                     kernel_L, r, k_i, k_f, idx_limit,
                     idx_limit_r2=N_grid,
                     method="filon", w_grid=w,
-                    n_nodes=CC_nodes, phase_increment=phase_increment
+                    n_nodes=CC_nodes, phase_increment=phase_increment,
+                    eta_total=eta_total
                 )
                 
                 # --- I_out: Pure oscillatory [r_m, r_max] via Levin/Filon ---
@@ -737,7 +939,7 @@ def radial_ME_all_L(
                 # --- I_in: Use legacy CC quadrature for inner region ---
                 I_in = oscillatory_kernel_integral_2d(
                     rho1_dir_uw, rho2_dir_uw, kernel_L, r, k_i, k_f, idx_limit, method="filon", w_grid=w,
-                    n_nodes=CC_nodes, phase_increment=phase_increment
+                    n_nodes=CC_nodes, phase_increment=phase_increment, eta_total=eta_total
                 )
                 
                 # --- I_out: Outer tail integral on [r_m, r_max] ---
@@ -816,7 +1018,8 @@ def radial_ME_all_L(
             I_ex = oscillatory_kernel_integral_2d(
                 rho1_ex_uw, rho2_ex_uw, kernel_L, r, k_i, k_f, idx_limit,
                 method="filon_exchange", w_grid=w,
-                n_nodes=CC_nodes, phase_increment=phase_increment
+                n_nodes=CC_nodes, phase_increment=phase_increment,
+                eta_total=eta_total
             )
         else:
             int_r2_ex = np.dot(kernel_L, rho2_ex_w)
@@ -947,7 +1150,18 @@ def _generate_gpu_filon_params(r_gpu, k_total, phase_increment=1.5708, n_nodes=5
         'n_nodes': n_nodes
     }
 
-def _gpu_filon_direct(rho1_uw, int_r2, r_gpu, w_gpu, k_total, phase_increment=1.5708, n_nodes=5, precomputed=None, return_gpu=False) -> float | cp.ndarray:
+def _gpu_filon_direct(
+    rho1_uw,
+    int_r2,
+    r_gpu,
+    w_gpu,
+    k_total,
+    phase_increment=1.5708,
+    n_nodes=5,
+    precomputed=None,
+    rho1_cc_pre=None,
+    return_gpu=False
+) -> float | cp.ndarray:
     """
     GPU Filon quadrature for direct integral outer loop.
     
@@ -975,7 +1189,10 @@ def _gpu_filon_direct(rho1_uw, int_r2, r_gpu, w_gpu, k_total, phase_increment=1.
     n_nodes = params['n_nodes']
     
     # OPTIMIZED: Pure GPU interpolation
-    rho1_cc = cp.interp(all_r_flat, r_gpu, rho1_uw).reshape(n_valid, n_nodes)
+    if rho1_cc_pre is not None:
+        rho1_cc = rho1_cc_pre
+    else:
+        rho1_cc = cp.interp(all_r_flat, r_gpu, rho1_uw).reshape(n_valid, n_nodes)
     int_r2_cc = cp.interp(all_r_flat, r_gpu, int_r2).reshape(n_valid, n_nodes)
     
     integrand = rho1_cc * int_r2_cc
@@ -986,7 +1203,20 @@ def _gpu_filon_direct(rho1_uw, int_r2, r_gpu, w_gpu, k_total, phase_increment=1.
 
 
 
-def _gpu_filon_exchange(kernel_L, rho1_uw, rho2_uw, r_gpu, w_gpu, k_total, phase_increment=1.5708, n_nodes=5, precomputed=None, return_gpu=False) -> float | cp.ndarray:
+def _gpu_filon_exchange(
+    kernel_L,
+    rho1_uw,
+    rho2_uw,
+    r_gpu,
+    w_gpu,
+    k_total,
+    phase_increment=1.5708,
+    n_nodes=5,
+    precomputed=None,
+    rho1_cc_pre=None,
+    rho2_cc_pre=None,
+    return_gpu=False
+) -> float | cp.ndarray:
     """
     GPU Filon quadrature for exchange integral (CC on both inner and outer).
     
@@ -1022,21 +1252,31 @@ def _gpu_filon_exchange(kernel_L, rho1_uw, rho2_uw, r_gpu, w_gpu, k_total, phase
     
     # Kernel interpolation: reconstruct kernel at CC nodes
     if isinstance(kernel_L, dict):
-        # Optimized path: use pre-sliced components
-        inv_gtr_at_cc = kernel_L['inv_gtr_at_cc']
-        log_ratio_at_cc = kernel_L['log_ratio_at_cc']
-        L = kernel_L['L']
-        if L == 0:
-            kernel_at_cc = inv_gtr_at_cc
+        # Optimized path: use precomputed kernel at CC nodes if provided.
+        if 'kernel_at_cc' in kernel_L:
+            kernel_at_cc = kernel_L['kernel_at_cc']
         else:
-            kernel_at_cc = inv_gtr_at_cc * cp.exp(L * log_ratio_at_cc)
+            inv_gtr_at_cc = kernel_L['inv_gtr_at_cc']
+            L = kernel_L['L']
+            if L == 0:
+                kernel_at_cc = inv_gtr_at_cc
+            else:
+                if 'ratio_at_cc' in kernel_L:
+                    ratio_at_cc = kernel_L['ratio_at_cc']
+                    kernel_at_cc = inv_gtr_at_cc * cp.power(ratio_at_cc, L)
+                else:
+                    log_ratio_at_cc = kernel_L['log_ratio_at_cc']
+                    kernel_at_cc = inv_gtr_at_cc * cp.exp(L * log_ratio_at_cc)
     else:
         # Standard path: slice from full matrix
         kernel_at_cc = kernel_L[:, idx_left] * weight_left + kernel_L[:, idx_right] * weight_right
     kernel_interp = kernel_at_cc.reshape(n_r, n_valid, n_nodes)
     
     # OPTIMIZED: Pure GPU interpolation for rho2
-    rho2_cc = cp.interp(all_r_flat, r_gpu, rho2_uw).reshape(n_valid, n_nodes)
+    if rho2_cc_pre is not None:
+        rho2_cc = rho2_cc_pre
+    else:
+        rho2_cc = cp.interp(all_r_flat, r_gpu, rho2_uw).reshape(n_valid, n_nodes)
 
     # Inner integral: for each r1, sum over CC nodes with weights
     inner_integrand = kernel_interp * rho2_cc[None, :, :]
@@ -1045,7 +1285,10 @@ def _gpu_filon_exchange(kernel_L, rho1_uw, rho2_uw, r_gpu, w_gpu, k_total, phase
     int_r2_cc = cp.sum(inner_integrand * weights_scaled[None, :, :], axis=(1, 2))
     
     # OPTIMIZED: Pure GPU interpolation for outer integral
-    rho1_cc = cp.interp(all_r_flat, r_gpu, rho1_uw).reshape(n_valid, n_nodes)
+    if rho1_cc_pre is not None:
+        rho1_cc = rho1_cc_pre
+    else:
+        rho1_cc = cp.interp(all_r_flat, r_gpu, rho1_uw).reshape(n_valid, n_nodes)
     int_r2_outer = cp.interp(all_r_flat, r_gpu, int_r2_cc).reshape(n_valid, n_nodes)
     
     outer_integrand = rho1_cc * int_r2_outer
@@ -1107,17 +1350,19 @@ def radial_ME_all_L_gpu(
     N_grid = len(r)
     idx_limit = N_grid
     
-    # === DEBUG LOGGING: Capture all input sizes ===
-    logger.debug(
-        "radial_ME_all_L_gpu ENTRY | N_grid=%d | V_core=%d, U_i=%d | "
-        "bound_i.u=%d, bound_f.u=%d | cont_i.chi=%d (idx_m=%d), cont_f.chi=%d (idx_m=%d)",
-        N_grid, len(V_core_array), len(U_i_array),
-        len(bound_i.u_of_r), len(bound_f.u_of_r),
-        len(cont_i.chi_of_r), cont_i.idx_match,
-        len(cont_f.chi_of_r), cont_f.idx_match
-    )
-    if U_f_array is not None:
-        logger.debug("radial_ME_all_L_gpu ENTRY | U_f=%d", len(U_f_array))
+    # Very chatty hot-path debug is sampled by default; set DWBA_HOTPATH_DEBUG=1
+    # for full per-call diagnostics.
+    if _should_sampled_hotpath_debug("radial_me_entry", every=250, initial=2):
+        logger.debug(
+            "radial_ME_all_L_gpu ENTRY | N_grid=%d | V_core=%d, U_i=%d | "
+            "bound_i.u=%d, bound_f.u=%d | cont_i.chi=%d (idx_m=%d), cont_f.chi=%d (idx_m=%d)",
+            N_grid, len(V_core_array), len(U_i_array),
+            len(bound_i.u_of_r), len(bound_f.u_of_r),
+            len(cont_i.chi_of_r), cont_i.idx_match,
+            len(cont_f.chi_of_r), cont_f.idx_match
+        )
+        if U_f_array is not None:
+            logger.debug("radial_ME_all_L_gpu ENTRY | U_f=%d", len(U_f_array))
     
     # ==========================================================================
     # ARRAY SIZE VALIDATION (v2.9+: LOCAL adaptive mode safety)
@@ -1228,7 +1473,10 @@ def radial_ME_all_L_gpu(
         u_i_raw = u_i_raw[:N_grid]
     elif len(u_i_raw) < N_grid:
         raise ValueError(f"Bound state u_i has {len(u_i_raw)} pts but grid has {N_grid} - cannot interpolate")
-    u_i_full_gpu = cp.asarray(u_i_raw)
+    if gpu_cache is not None:
+        u_i_full_gpu = gpu_cache.get_static_array(u_i_raw, "u_i", n_grid=N_grid)
+    else:
+        u_i_full_gpu = cp.asarray(u_i_raw)
     
     if isinstance(bound_f, BoundOrbital):
         u_f_raw = bound_f.u_of_r
@@ -1237,13 +1485,17 @@ def radial_ME_all_L_gpu(
             u_f_raw = u_f_raw[:N_grid]
         elif len(u_f_raw) < N_grid:
             raise ValueError(f"Bound state u_f has {len(u_f_raw)} pts but grid has {N_grid} - cannot interpolate")
-        u_f_full_gpu = cp.asarray(u_f_raw)
+        if gpu_cache is not None:
+            u_f_full_gpu = gpu_cache.get_static_array(u_f_raw, "u_f_bound", n_grid=N_grid)
+        else:
+            u_f_full_gpu = cp.asarray(u_f_raw)
     elif hasattr(bound_f, 'chi_of_r'):
         u_f_raw = bound_f.chi_of_r
         if len(u_f_raw) > N_grid:
             u_f_raw = u_f_raw[:N_grid]
         elif len(u_f_raw) < N_grid:
             raise ValueError(f"Continuum u_f has {len(u_f_raw)} pts but grid has {N_grid}")
+        # Ionization continuum can vary per call; avoid cache churn here.
         u_f_full_gpu = cp.asarray(u_f_raw)  # Ionization path
     else:
         u_f_full_gpu = u_i_full_gpu  # Fallback
@@ -1271,7 +1523,10 @@ def radial_ME_all_L_gpu(
         chi_i_gpu = cp.asarray(cont_i.chi_of_r[:idx_limit])
         chi_f_gpu = cp.asarray(cont_f.chi_of_r[:idx_limit])
     
-    V_diff_gpu = cp.asarray((V_core_array - U_i_array)[:idx_limit])
+    if gpu_cache is not None:
+        V_diff_gpu = gpu_cache.get_v_diff(V_core_array, U_i_array, idx_limit)
+    else:
+        V_diff_gpu = cp.asarray((V_core_array - U_i_array)[:idx_limit])
 
     # 2. Precompute Densities on GPU
     # Weighted (for standard method / L=0 correction)
@@ -1295,26 +1550,70 @@ def radial_ME_all_L_gpu(
     
     # Memory-based decision for matrix construction
     use_block_wise = False
+    prefer_filon_full = False
+    effective_free_mem = None
     if gpu_memory_mode == "block":
         use_block_wise = True
     elif gpu_memory_mode == "full":
         use_block_wise = False
+        prefer_filon_full = use_filon
     else:  # "auto" - check GPU memory
         try:
-            free_mem, total_mem = cp.cuda.Device().mem_info
+            free_mem, _total_mem = cp.cuda.Device().mem_info
+
+            # Include reusable bytes already reserved in CuPy pool.
+            # Device free memory alone can be pessimistic after many iterations.
+            mem_pool = cp.get_default_memory_pool()
+            pool_total = mem_pool.total_bytes()
+            pool_used = mem_pool.used_bytes()
+            pool_reusable = max(0, pool_total - pool_used)
+            effective_free = free_mem + pool_reusable
+            effective_free_mem = int(effective_free)
+
+            # If base kernel is already cached for this idx_limit, don't count it again.
+            has_cached_base_kernel = (
+                gpu_cache is not None and
+                gpu_cache.inv_gtr is not None and
+                gpu_cache.log_ratio is not None and
+                gpu_cache.idx_limit >= idx_limit
+            )
+            standard_mem = 0 if has_cached_base_kernel else (idx_limit * idx_limit * 8 * 3)
+            mem_budget = effective_free * gpu_memory_threshold
+
             # Memory estimation depends on mode:
             # - Standard: 3 matrices (inv_gtr, ratio, log_ratio) ├Ś idx_limit┬▓
             # - Filon: Extended matrix (idx_limit ├Ś N_grid) + standard
             if use_filon:
-                # Extended Filon matrix + standard matrix
-                required_mem = (idx_limit * N_grid * 8 * 2) + (idx_limit * idx_limit * 8 * 3)
+                # For full Filon path, standard kernel is not mandatory.
+                has_cached_filon_kernel = (
+                    gpu_cache is not None and
+                    gpu_cache.filon_inv_gtr is not None and
+                    gpu_cache.filon_log_ratio is not None and
+                    gpu_cache.filon_idx_limit >= idx_limit
+                )
+                filon_full_mem = 0 if has_cached_filon_kernel else (idx_limit * N_grid * 8 * 2)
+                if filon_full_mem <= mem_budget:
+                    prefer_filon_full = True
+                elif standard_mem <= mem_budget:
+                    # Not enough for full Filon matrix, but enough for hybrid.
+                    prefer_filon_full = False
+                else:
+                    use_block_wise = True
             else:
-                required_mem = idx_limit * idx_limit * 8 * 3
-            
-            if required_mem > free_mem * gpu_memory_threshold:
-                logger.info("GPU memory limited (%.1f GB free, need %.1f GB). Using block-wise.",
-                           free_mem / 1e9, required_mem / 1e9)
-                use_block_wise = True
+                required_mem = standard_mem
+                if required_mem > mem_budget:
+                    use_block_wise = True
+
+            if use_block_wise:
+                logger.info(
+                    "GPU memory limited (free %.1f GB + pool %.1f GB, need %.1f GB). Using block-wise.",
+                    free_mem / 1e9,
+                    pool_reusable / 1e9,
+                    (
+                        (idx_limit * N_grid * 8 * 2) if use_filon
+                        else standard_mem
+                    ) / 1e9
+                )
         except Exception as e:
             logger.debug("Could not check GPU memory: %s. Using block-wise as fallback.", e)
             use_block_wise = True
@@ -1331,22 +1630,26 @@ def radial_ME_all_L_gpu(
     inv_gtr = None
     log_ratio = None
     
-    if not use_block_wise:
-        # Try to build full kernel matrix (works for BOTH Filon and standard)
-        cp.get_default_memory_pool().free_all_blocks()
-        gc.collect()
-        
+    if not use_block_wise and (not use_filon or not prefer_filon_full):
+        # Try to build/reuse full kernel matrix (works for BOTH Filon and standard).
+        # IMPORTANT: avoid unconditional memory-pool flush in hot path; it introduces
+        # frequent host-device synchronization and defeats allocator reuse.
         try:
-            r_col = r_gpu[:, None]
-            r_row = r_gpu[None, :]
-            inv_gtr = 1.0 / cp.maximum(r_col, r_row + 1e-30)
-            ratio = cp.minimum(r_col, r_row) * inv_gtr
-            ratio = cp.minimum(ratio, 1.0 - 1e-12)
-            log_ratio = cp.log(ratio + 1e-30)
-            del r_col, r_row, ratio
-            cp.get_default_memory_pool().free_all_blocks()
+            if gpu_cache is not None:
+                gpu_cache.build_kernel_matrix(idx_limit)
+                inv_gtr = gpu_cache.inv_gtr[:idx_limit, :idx_limit]
+                log_ratio = gpu_cache.log_ratio[:idx_limit, :idx_limit]
+            else:
+                r_col = r_gpu[:, None]
+                r_row = r_gpu[None, :]
+                inv_gtr = 1.0 / cp.maximum(r_col, r_row + 1e-30)
+                ratio = cp.minimum(r_col, r_row) * inv_gtr
+                ratio = cp.minimum(ratio, 1.0 - 1e-12)
+                log_ratio = cp.log(ratio + 1e-30)
+                del r_col, r_row, ratio
             full_matrix_built = True
-            logger.debug("GPU: Standard matrix built (%d├Ś%d)", idx_limit, idx_limit)
+            if _should_sampled_hotpath_debug("standard_matrix_ready", every=200, initial=2):
+                logger.debug("GPU: Standard matrix ready (%d├Ś%d)", idx_limit, idx_limit)
         except Exception as e:
             # Catch OutOfMemoryError and Windows pagefile errors
             err_str = str(e).lower()
@@ -1355,7 +1658,6 @@ def radial_ME_all_L_gpu(
                 use_block_wise = True
                 full_matrix_built = False
                 cp.get_default_memory_pool().free_all_blocks()
-                gc.collect()
             else:
                 raise
     
@@ -1365,60 +1667,121 @@ def radial_ME_all_L_gpu(
     filon_inv_gtr = None
     filon_log_ratio = None
     
-    if use_filon and not use_block_wise:
-        cp.get_default_memory_pool().free_all_blocks()
-        gc.collect()
-        
+    if use_filon and not use_block_wise and prefer_filon_full:
         try:
-            r_col = r_gpu[:, None]  # (idx_limit, 1)
-            r_row_full = r_full_gpu[None, :]  # (1, N_grid)
-            filon_inv_gtr = 1.0 / cp.maximum(r_col, r_row_full + 1e-30)
-            filon_ratio = cp.minimum(r_col, r_row_full) * filon_inv_gtr
-            filon_ratio = cp.minimum(filon_ratio, 1.0 - 1e-12)
-            filon_log_ratio = cp.log(filon_ratio + 1e-30)
-            del r_col, r_row_full, filon_ratio
-            cp.get_default_memory_pool().free_all_blocks()
+            if gpu_cache is not None:
+                gpu_cache.build_filon_kernel_matrix(idx_limit)
+                filon_inv_gtr = gpu_cache.filon_inv_gtr[:idx_limit, :]
+                filon_log_ratio = gpu_cache.filon_log_ratio[:idx_limit, :]
+            else:
+                r_col = r_gpu[:, None]  # (idx_limit, 1)
+                r_row_full = r_full_gpu[None, :]  # (1, N_grid)
+                filon_inv_gtr = 1.0 / cp.maximum(r_col, r_row_full + 1e-30)
+                filon_ratio = cp.minimum(r_col, r_row_full) * filon_inv_gtr
+                filon_ratio = cp.minimum(filon_ratio, 1.0 - 1e-12)
+                filon_log_ratio = cp.log(filon_ratio + 1e-30)
+                del r_col, r_row_full, filon_ratio
             filon_kernel_built = True
-            logger.debug("GPU: Filon extended matrix built (%d├Ś%d)", idx_limit, N_grid)
+            if _should_sampled_hotpath_debug("filon_matrix_build", every=200, initial=2):
+                logger.debug("GPU: Filon extended matrix ready (%d├Ś%d)", idx_limit, N_grid)
         except Exception as e:
             err_str = str(e).lower()
             if "memory" in err_str or "pagefile" in err_str or "out of memory" in err_str:
                 logger.info("GPU: Filon extended matrix too large, using hybrid mode")
                 filon_kernel_built = False
                 cp.get_default_memory_pool().free_all_blocks()
-                gc.collect()
+            else:
+                raise
+
+    # Hybrid fallback: if full Filon matrix was preferred but not built, we still
+    # need the standard kernel matrix for head-dot and exchange fallbacks.
+    if use_filon and not use_block_wise and not filon_kernel_built and not full_matrix_built:
+        try:
+            if gpu_cache is not None:
+                gpu_cache.build_kernel_matrix(idx_limit)
+                inv_gtr = gpu_cache.inv_gtr[:idx_limit, :idx_limit]
+                log_ratio = gpu_cache.log_ratio[:idx_limit, :idx_limit]
+            else:
+                r_col = r_gpu[:, None]
+                r_row = r_gpu[None, :]
+                inv_gtr = 1.0 / cp.maximum(r_col, r_row + 1e-30)
+                ratio = cp.minimum(r_col, r_row) * inv_gtr
+                ratio = cp.minimum(ratio, 1.0 - 1e-12)
+                log_ratio = cp.log(ratio + 1e-30)
+                del r_col, r_row, ratio
+            full_matrix_built = True
+            if _should_sampled_hotpath_debug("standard_matrix_hybrid_ready", every=200, initial=2):
+                logger.debug("GPU: Standard matrix ready for hybrid fallback (%d├Ś%d)", idx_limit, idx_limit)
+        except Exception as e:
+            err_str = str(e).lower()
+            if "memory" in err_str or "pagefile" in err_str or "out of memory" in err_str:
+                logger.warning("GPU hybrid fallback allocation failed: %s. Switching to block-wise.", e)
+                use_block_wise = True
+                full_matrix_built = False
+                cp.get_default_memory_pool().free_all_blocks()
             else:
                 raise
     
     # Filon params for oscillatory quadrature (only if use_filon)
     filon_params = None
+    kernel_at_cc_pre = None
     if use_filon:
-        filon_params = _generate_gpu_filon_params(r_gpu, k_total, phase_increment, CC_nodes)
+        filon_key = (
+            int(idx_limit),
+            float(np.round(k_total, 12)),
+            float(np.round(phase_increment, 8)),
+            int(CC_nodes),
+        )
+        if gpu_cache is not None and isinstance(gpu_cache.filon_params, dict):
+            if gpu_cache.filon_params.get("key") == filon_key:
+                filon_params = gpu_cache.filon_params.get("params")
+
+        if filon_params is None:
+            filon_params = _generate_gpu_filon_params(r_gpu, k_total, phase_increment, CC_nodes)
+            if gpu_cache is not None:
+                gpu_cache.filon_params = {"key": filon_key, "params": filon_params}
+
         if filon_params:
-            # Precompute CC node kernels for Filon
-            idx_l, idx_r = filon_params['idx_left'], filon_params['idx_right']
-            w_l, w_r = filon_params['w_left'], filon_params['w_right']
-            
-            r_col = r_gpu[:, None]
-            r_cc_l = r_gpu[idx_l][None, :]
-            r_cc_r = r_gpu[idx_r][None, :]
-            
-            inv_gtr_cc_l = 1.0 / cp.maximum(r_col, r_cc_l + 1e-30)
-            log_ratio_cc_l = cp.log(cp.minimum(r_col, r_cc_l) * inv_gtr_cc_l + 1e-30)
-            
-            inv_gtr_cc_r = 1.0 / cp.maximum(r_col, r_cc_r + 1e-30)
-            log_ratio_cc_r = cp.log(cp.minimum(r_col, r_cc_r) * inv_gtr_cc_r + 1e-30)
-            
-            inv_gtr_at_cc = inv_gtr_cc_l * w_l + inv_gtr_cc_r * w_r
-            log_ratio_at_cc = log_ratio_cc_l * w_l + log_ratio_cc_r * w_r
-            
-            kernel_at_cc_pre = {
-                'inv_gtr_at_cc': inv_gtr_at_cc,
-                'log_ratio_at_cc': log_ratio_at_cc
-            }
-            del r_col, r_cc_l, r_cc_r, inv_gtr_cc_l, inv_gtr_cc_r, log_ratio_cc_l, log_ratio_cc_r
-            gc.collect()
-            cp.get_default_memory_pool().free_all_blocks()
+            kernel_cc_key = filon_key
+            if (
+                gpu_cache is not None and
+                gpu_cache.kernel_cc_pre is not None and
+                gpu_cache.kernel_cc_key == kernel_cc_key
+            ):
+                kernel_at_cc_pre = gpu_cache.kernel_cc_pre
+            else:
+                # Precompute CC node kernels for Filon
+                idx_l, idx_r = filon_params['idx_left'], filon_params['idx_right']
+                w_l, w_r = filon_params['w_left'], filon_params['w_right']
+
+                r_col = r_gpu[:, None]
+                r_cc_l = r_gpu[idx_l][None, :]
+                r_cc_r = r_gpu[idx_r][None, :]
+
+                inv_gtr_cc_l = 1.0 / cp.maximum(r_col, r_cc_l + 1e-30)
+                log_ratio_cc_l = cp.log(cp.minimum(r_col, r_cc_l) * inv_gtr_cc_l + 1e-30)
+
+                inv_gtr_cc_r = 1.0 / cp.maximum(r_col, r_cc_r + 1e-30)
+                log_ratio_cc_r = cp.log(cp.minimum(r_col, r_cc_r) * inv_gtr_cc_r + 1e-30)
+
+                inv_gtr_at_cc = inv_gtr_cc_l * w_l + inv_gtr_cc_r * w_r
+                log_ratio_at_cc = log_ratio_cc_l * w_l + log_ratio_cc_r * w_r
+                try:
+                    ratio_at_cc = cp.exp(log_ratio_at_cc)
+                    kernel_at_cc_pre = {
+                        'inv_gtr_at_cc': inv_gtr_at_cc,
+                        'ratio_at_cc': ratio_at_cc
+                    }
+                except Exception:
+                    # Memory-constrained fallback: keep log-ratio and evaluate per L.
+                    kernel_at_cc_pre = {
+                        'inv_gtr_at_cc': inv_gtr_at_cc,
+                        'log_ratio_at_cc': log_ratio_at_cc
+                    }
+                del r_col, r_cc_l, r_cc_r, inv_gtr_cc_l, inv_gtr_cc_r, log_ratio_cc_l, log_ratio_cc_r
+                if gpu_cache is not None:
+                    gpu_cache.kernel_cc_pre = kernel_at_cc_pre
+                    gpu_cache.kernel_cc_key = kernel_cc_key
         else:
             use_filon = False  # Fallback if params generation failed
     
@@ -1445,12 +1808,22 @@ def radial_ME_all_L_gpu(
     I_L_exc_gpu = cp.zeros(L_max + 1, dtype=cp.float64)
 
     # Block size for block-wise computation (only used if needed)
-    # Handle string "auto" or int values
+    # Handle string "auto" or int values.
+    # Avoid auto-tuning when no block-wise path is used (saves overhead and log noise).
     try:
         block_size_int = int(gpu_block_size) if gpu_block_size != "auto" else 0
     except (ValueError, TypeError):
         block_size_int = 0
-    BLOCK_SIZE = block_size_int if block_size_int > 0 else _compute_optimal_block_size(idx_limit, gpu_memory_threshold)
+    need_block_loops = use_block_wise or (use_filon and not filon_kernel_built and N_grid > idx_limit)
+    BLOCK_SIZE = None
+    if need_block_loops:
+        BLOCK_SIZE = block_size_int if block_size_int > 0 else _compute_optimal_block_size(
+            idx_limit,
+            gpu_memory_threshold,
+            effective_free_mem=effective_free_mem
+        )
+        if BLOCK_SIZE is None or BLOCK_SIZE <= 0:
+            BLOCK_SIZE = 2048
     
     # Precompute rho2_eff_full ONCE before L-loop (optimization: was computed per-L)
     rho2_eff_full = (u_f_full_gpu * u_i_full_gpu) * w_full_gpu if use_filon else None
@@ -1461,16 +1834,94 @@ def radial_ME_all_L_gpu(
     V_diff_dot_rho1_dir = cp.dot(rho1_dir_w, V_diff_gpu)
     V_diff_dot_rho1_ex = cp.dot(rho1_ex_w, V_diff_gpu)
 
+    is_excitation = isinstance(bound_f, BoundOrbital)
+
+    # Precompute bound multipole moments once on CPU to avoid per-L GPU syncs.
+    moments_head = None
+    moments_full = None
+    if use_filon and is_excitation and idx_limit < N_grid - 10 and L_max >= 1:
+        r_head = r[:idx_limit]
+        r_full = r[:N_grid]
+        w_head = grid.w_simpson[:idx_limit]
+        w_full = grid.w_simpson[:N_grid]
+
+        base_head = w_head * u_f_raw[:idx_limit] * u_i_raw[:idx_limit]
+        base_full = w_full * u_f_raw[:N_grid] * u_i_raw[:N_grid]
+
+        moments_head = np.zeros(L_max + 1, dtype=float)
+        moments_full = np.zeros(L_max + 1, dtype=float)
+
+        r_pow_head = np.ones_like(r_head)
+        r_pow_full = np.ones_like(r_full)
+        for Lm in range(1, L_max + 1):
+            r_pow_head *= r_head
+            r_pow_full *= r_full
+            moments_head[Lm] = float(np.dot(base_head, r_pow_head))
+            moments_full[Lm] = float(np.dot(base_full, r_pow_full))
+
+    # Precompute ratio matrices once; update powers recursively in the L-loop.
+    need_standard_kernel_for_direct = (not use_filon) or (use_filon and not filon_kernel_built)
+    need_standard_kernel_for_exchange = not (
+        use_oscillatory_quadrature and k_total > k_threshold and kernel_at_cc_pre is not None
+    )
+    need_standard_kernel = need_standard_kernel_for_direct or need_standard_kernel_for_exchange
+
+    kernel_ratio = None
+    kernel_ratio_pow = None
+    if full_matrix_built and need_standard_kernel and L_max > 0:
+        try:
+            if gpu_cache is not None:
+                kernel_ratio = gpu_cache.get_kernel_ratio(idx_limit)
+            if kernel_ratio is None:
+                kernel_ratio = cp.exp(log_ratio)
+        except Exception:
+            kernel_ratio = None
+
+    filon_ratio = None
+    filon_ratio_pow = None
+    if filon_kernel_built and L_max > 0:
+        try:
+            if gpu_cache is not None:
+                filon_ratio = gpu_cache.get_filon_ratio(idx_limit)
+            if filon_ratio is None:
+                filon_ratio = cp.exp(filon_log_ratio)
+        except Exception:
+            filon_ratio = None
+
+    cc_ratio = kernel_at_cc_pre.get('ratio_at_cc') if (kernel_at_cc_pre is not None and L_max > 0) else None
+    cc_ratio_pow = None
+
+    # Precompute CC interpolations reused across all L for Filon outer loops.
+    rho1_dir_cc_pre = None
+    rho1_ex_cc_pre = None
+    rho2_ex_cc_pre = None
+    if use_filon and filon_params:
+        all_r_flat = filon_params['all_r_flat']
+        n_valid = filon_params['n_valid']
+        n_nodes = filon_params['n_nodes']
+        rho1_dir_cc_pre = cp.interp(all_r_flat, r_gpu, rho1_dir_uw).reshape(n_valid, n_nodes)
+        if use_oscillatory_quadrature and k_total > k_threshold:
+            rho1_ex_cc_pre = cp.interp(all_r_flat, r_gpu, rho1_ex_uw).reshape(n_valid, n_nodes)
+            rho2_ex_cc_pre = cp.interp(all_r_flat, r_gpu, rho2_ex_uw).reshape(n_valid, n_nodes)
+
     for L in range(L_max + 1):
         kernel_L = None
         int_r2_dir = None  # For Filon mode
         
-        if full_matrix_built:
-            # FAST PATH: use prebuilt full matrix
-            if L == 0:
-                kernel_L = inv_gtr
-            else:
-                kernel_L = inv_gtr * cp.exp(L * log_ratio)
+        if full_matrix_built or filon_kernel_built:
+            # FAST PATH: use prebuilt matrices.
+            if full_matrix_built and need_standard_kernel:
+                if L == 0:
+                    kernel_L = inv_gtr
+                else:
+                    if kernel_ratio is None:
+                        kernel_L = inv_gtr * cp.exp(L * log_ratio)
+                    else:
+                        if kernel_ratio_pow is None:
+                            kernel_ratio_pow = kernel_ratio.copy()
+                        else:
+                            kernel_ratio_pow *= kernel_ratio
+                        kernel_L = inv_gtr * kernel_ratio_pow
             
             # For Filon mode, compute int_r2_dir:
             # FAST: If filon_kernel_built, use full (idx_limit ├Ś N_grid) matrix
@@ -1483,20 +1934,30 @@ def radial_ME_all_L_gpu(
                     if L == 0:
                         filon_kernel_L = filon_inv_gtr
                     else:
-                        filon_kernel_L = filon_inv_gtr * cp.exp(L * filon_log_ratio)
+                        if filon_ratio is None:
+                            filon_kernel_L = filon_inv_gtr * cp.exp(L * filon_log_ratio)
+                        else:
+                            if filon_ratio_pow is None:
+                                filon_ratio_pow = filon_ratio.copy()
+                            else:
+                                filon_ratio_pow *= filon_ratio
+                            filon_kernel_L = filon_inv_gtr * filon_ratio_pow
                     int_r2_dir = cp.dot(filon_kernel_L, rho2_eff_full)
                     del filon_kernel_L
                 else:
                     # HYBRID PATH: Use standard matrix for head + block-wise for tail
+                    if kernel_L is None:
+                        raise RuntimeError("Hybrid Filon path requires standard kernel_L, but it is not available.")
                     int_r2_dir = cp.dot(kernel_L, rho2_eff_full[:idx_limit])
                     
                     if N_grid > idx_limit:
                         r_col = r_gpu[:, None]
                         r_row_tail = r_full_gpu[idx_limit:][None, :]
                         tail_size = N_grid - idx_limit
+                        block_step = BLOCK_SIZE if BLOCK_SIZE is not None else 2048
                         
-                        for start in range(0, tail_size, BLOCK_SIZE):
-                            end = min(start + BLOCK_SIZE, tail_size)
+                        for start in range(0, tail_size, block_step):
+                            end = min(start + block_step, tail_size)
                             r_block = r_row_tail[:, start:end]
                             inv_gtr_b = 1.0 / cp.maximum(r_col, r_block + 1e-30)
                             if L == 0:
@@ -1514,8 +1975,9 @@ def radial_ME_all_L_gpu(
                 int_r2_dir = cp.zeros(idx_limit, dtype=float)
                 r_col = r_gpu[:, None]
                 r_row_full = r_full_gpu[None, :]
-                for start in range(0, N_grid, BLOCK_SIZE):
-                    end = min(start + BLOCK_SIZE, N_grid)
+                block_step = BLOCK_SIZE if BLOCK_SIZE is not None else 2048
+                for start in range(0, N_grid, block_step):
+                    end = min(start + block_step, N_grid)
                     inv_gtr_b = 1.0 / cp.maximum(r_col, r_row_full[:, start:end] + 1e-30)
                     if L == 0:
                         kb = inv_gtr_b
@@ -1542,19 +2004,29 @@ def radial_ME_all_L_gpu(
 
         
         # --- Direct Integral on GPU ---
-        is_excitation = isinstance(bound_f, BoundOrbital)
         
         if use_filon:
             # oscillatory_method: legacy, full_split, or advanced
             # int_r2_dir was already computed block-wise above to save memory
             # NOTE: _gpu_filon_direct returns a GPU scalar, not Python float
-            I_dir_L = _gpu_filon_direct(rho1_dir_uw, int_r2_dir, r_gpu, w_gpu, k_total, phase_increment, CC_nodes, precomputed=filon_params, return_gpu=True)
+            I_dir_L = _gpu_filon_direct(
+                rho1_dir_uw,
+                int_r2_dir,
+                r_gpu,
+                w_gpu,
+                k_total,
+                phase_increment,
+                CC_nodes,
+                precomputed=filon_params,
+                rho1_cc_pre=rho1_dir_cc_pre,
+                return_gpu=True
+            )
             
             if oscillatory_method == "legacy":
                 # Add analytical tail if applicable (requires CPU computation)
                 # For legacy mode, we still need float conversion for tail
                 if L >= 1 and idx_limit < N_grid - 10 and is_excitation:
-                    moment_L = float(cp.sum(w_gpu * (r_gpu ** L) * u_f_gpu * u_i_gpu))
+                    moment_L = moments_head[L] if moments_head is not None else 0.0
                     if abs(moment_L) > 1e-12:
                         tail_contrib = _analytical_multipole_tail(
                             r_match, k_i, k_f, delta_i, delta_f, l_i, l_f, L, moment_L,
@@ -1565,7 +2037,7 @@ def radial_ME_all_L_gpu(
                 # full_split or advanced: both use Levin/Filon for tail
                 if L >= 1 and idx_limit < N_grid - 10 and is_excitation:
                     # Multipole moment from bound states (use full grid for accuracy)
-                    moment_L = float(cp.sum(w_full_gpu * (r_full_gpu ** L) * u_f_full_gpu * u_i_full_gpu))
+                    moment_L = moments_full[L] if moments_full is not None else 0.0
                     if abs(moment_L) > 1e-12:
                         def make_envelope(mL, Lval):
                             def envelope(r_val):
@@ -1606,10 +2078,27 @@ def radial_ME_all_L_gpu(
         if use_oscillatory_quadrature and k_total > k_threshold:
             # Use GPU Filon + CC for exchange (both inner and outer)
             # Pass pre-sliced components instead of full matrix to save memory
-            k_spec = {**kernel_at_cc_pre, 'L': L} if 'kernel_at_cc_pre' in locals() else kernel_L
+            if kernel_at_cc_pre is not None:
+                if L == 0:
+                    k_spec = {'kernel_at_cc': kernel_at_cc_pre['inv_gtr_at_cc']}
+                else:
+                    if cc_ratio is None:
+                        k_spec = {**kernel_at_cc_pre, 'L': L}
+                    else:
+                        if cc_ratio_pow is None:
+                            cc_ratio_pow = cc_ratio.copy()
+                        else:
+                            cc_ratio_pow *= cc_ratio
+                        k_spec = {
+                            'kernel_at_cc': kernel_at_cc_pre['inv_gtr_at_cc'] * cc_ratio_pow
+                        }
+            else:
+                k_spec = kernel_L
             I_ex_L = _gpu_filon_exchange(
                 k_spec, rho1_ex_uw, rho2_ex_uw, r_gpu, w_gpu, k_total, 
-                phase_increment, CC_nodes, precomputed=filon_params, return_gpu=True
+                phase_increment, CC_nodes, precomputed=filon_params,
+                rho1_cc_pre=rho1_ex_cc_pre, rho2_cc_pre=rho2_ex_cc_pre,
+                return_gpu=True
             )
         else:
             int_r2_ex = cp.dot(kernel_L, rho2_ex_w)
diff --git a/oscillatory_integrals.py b/oscillatory_integrals.py
index 0e5bcfb..a68f0f3 100644
--- a/oscillatory_integrals.py
+++ b/oscillatory_integrals.py
@@ -37,6 +37,9 @@ All inputs/outputs in Hartree atomic units (aÔéÇ, Ha).
 """
 
 from __future__ import annotations
+import logging
+import os
+import time
 import numpy as np
 from typing import Tuple, Optional
 from scipy.special import sici  # Sine and Cosine integrals
@@ -46,6 +49,61 @@ from logging_config import get_logger
 logger = get_logger(__name__)
 
 
+def _env_int(name: str, default: int, min_value: int = 1) -> int:
+    """Read integer env var with bounds and fallback."""
+    raw = os.environ.get(name, "").strip()
+    if not raw:
+        return max(min_value, int(default))
+    try:
+        return max(min_value, int(raw))
+    except Exception:
+        return max(min_value, int(default))
+
+
+def _env_float(name: str, default: float, min_value: float = 0.0) -> float:
+    """Read float env var with bounds and fallback."""
+    raw = os.environ.get(name, "").strip()
+    if not raw:
+        return max(min_value, float(default))
+    try:
+        return max(min_value, float(raw))
+    except Exception:
+        return max(min_value, float(default))
+
+
+# Runtime guardrails for pathological outer-tail costs.
+_FILON_MAX_SEGMENTS = _env_int("DWBA_MAX_FILON_SEGMENTS", 2048, min_value=128)
+_OUTER_SLOW_WARN_S = _env_float("DWBA_OUTER_SLOW_WARN_S", 20.0, min_value=1.0)
+
+
+def _is_hotpath_verbose_debug_enabled() -> bool:
+    """
+    Return True when per-segment oscillatory debug is explicitly requested.
+    """
+    return os.environ.get("DWBA_HOTPATH_DEBUG", "").strip().lower() in {"1", "true", "yes", "on"}
+
+
+def _should_sampled_debug(key: str, every: int = 500, initial: int = 2) -> bool:
+    """
+    Sample highly repetitive DEBUG logs in oscillatory inner loops.
+    """
+    if not logger.isEnabledFor(logging.DEBUG):
+        return False
+    if _is_hotpath_verbose_debug_enabled():
+        return True
+
+    counters = getattr(_should_sampled_debug, "_counters", None)
+    if counters is None:
+        counters = {}
+        _should_sampled_debug._counters = counters
+
+    count = counters.get(key, 0) + 1
+    counters[key] = count
+    if count <= initial:
+        return True
+    return (count % max(1, int(every))) == 0
+
+
 # =============================================================================
 # CACHED CLENSHAW-CURTIS REFERENCE WEIGHTS (OPTIMIZATION)
 # =============================================================================
@@ -79,6 +137,12 @@ _CC_W_REF = (2.0 / (_CC_N - 1)) * (1 - 2 * _weight_sums)  # Weights on [-1, 1]
 # Cache CC reference nodes/weights by node count
 _CC_CACHE = {_CC_N: (_CC_X_REF, _CC_W_REF)}
 
+# Cache for radial-grid spacing classification (linear vs log-like).
+# Keyed by memory pointer + basic shape/signature to avoid repeated O(N)
+# checks in tight interpolation loops.
+_LOG_GRID_FLAG_CACHE: dict[tuple[int, int, float, float], bool] = {}
+_LOG_GRID_FLAG_CACHE_MAX = 128
+
 
 def _get_cc_ref(n_nodes: int) -> Tuple[np.ndarray, np.ndarray]:
     """
@@ -139,6 +203,93 @@ def _kahan_sum_real(values: np.ndarray) -> float:
     return math.fsum(values)
 
 
+def _eval_callable_on_nodes(func, nodes: np.ndarray) -> np.ndarray:
+    """
+    Evaluate scalar/vector callable on nodes with fast array-first fallback.
+
+    Many envelope/phase callables are numpy-aware; trying array input first
+    avoids Python-loop overhead from np.vectorize in tight oscillatory loops.
+    """
+    try:
+        vals = func(nodes)
+        vals_arr = np.asarray(vals)
+        if vals_arr.shape == ():
+            vals_arr = np.full(nodes.shape, vals_arr, dtype=vals_arr.dtype)
+        elif vals_arr.shape != nodes.shape:
+            vals_arr = np.broadcast_to(vals_arr, nodes.shape)
+        return vals_arr
+    except Exception:
+        return np.vectorize(func)(nodes)
+
+
+def _is_log_spaced_grid(r: np.ndarray, rel_tol: float = 5e-3) -> bool:
+    """
+    Check whether a 1D positive grid is approximately uniform in log(r).
+    """
+    if r.ndim != 1 or len(r) < 3:
+        return False
+    if r[0] <= 0.0 or np.any(np.diff(r) <= 0.0):
+        return False
+    log_r = np.log(r)
+    dlog = np.diff(log_r)
+    mean_dlog = float(np.mean(dlog))
+    if not np.isfinite(mean_dlog) or mean_dlog <= 0.0:
+        return False
+    max_dev = float(np.max(np.abs(dlog - mean_dlog)))
+    return max_dev <= rel_tol * mean_dlog
+
+
+def _is_log_spaced_grid_cached(r: np.ndarray, rel_tol: float = 5e-3) -> bool:
+    """
+    Cached wrapper around _is_log_spaced_grid for repeated interpolation calls.
+    """
+    r_arr = np.asarray(r, dtype=float)
+    if r_arr.ndim != 1 or len(r_arr) < 3:
+        return False
+
+    key = (
+        int(r_arr.__array_interface__["data"][0]),
+        int(r_arr.size),
+        float(r_arr[0]),
+        float(r_arr[-1]),
+    )
+    cached = _LOG_GRID_FLAG_CACHE.get(key)
+    if cached is not None:
+        return cached
+
+    is_log = _is_log_spaced_grid(r_arr, rel_tol=rel_tol)
+    if len(_LOG_GRID_FLAG_CACHE) >= _LOG_GRID_FLAG_CACHE_MAX:
+        _LOG_GRID_FLAG_CACHE.clear()
+    _LOG_GRID_FLAG_CACHE[key] = is_log
+    return is_log
+
+
+def _interp_on_radial_grid(x_new: np.ndarray, x: np.ndarray, y: np.ndarray) -> np.ndarray:
+    """
+    1D interpolation aware of logarithmic radial grids.
+
+    For log-spaced positive grids, interpolation is performed in log(r) space
+    to reduce phase-envelope distortion compared with linear-r interpolation.
+    """
+    x_arr = np.asarray(x, dtype=float)
+    x_new_arr = np.asarray(x_new, dtype=float)
+    y_arr = np.asarray(y)
+
+    x_new_clip = np.clip(x_new_arr, x_arr[0], x_arr[-1])
+
+    if np.iscomplexobj(y_arr):
+        re = _interp_on_radial_grid(x_new_clip, x_arr, y_arr.real)
+        im = _interp_on_radial_grid(x_new_clip, x_arr, y_arr.imag)
+        return re + 1j * im
+
+    if _is_log_spaced_grid_cached(x_arr):
+        x_safe = np.maximum(x_arr, 1e-300)
+        x_new_safe = np.maximum(x_new_clip, x_safe[0])
+        return np.interp(np.log(x_new_safe), np.log(x_safe), y_arr)
+
+    return np.interp(x_new_clip, x_arr, y_arr)
+
+
 # =============================================================================
 # sinA ├Ś sinB DECOMPOSITION (Per Instruction)
 # =============================================================================
@@ -224,6 +375,8 @@ def dwba_outer_integral_1d(
     """
     if r_max <= r_m + 1e-10:
         return 0.0
+
+    t_outer_start = time.perf_counter()
     
     # Decompose into cos(╬Ž- ) and cos(╬Ž+ ) terms
     (k_minus, phi_c_minus, eta_minus), (k_plus, phi_c_plus, eta_plus) = compute_product_phases(
@@ -325,7 +478,16 @@ def dwba_outer_integral_1d(
     I_plus = I_plus_complex.real
     
     # Final result: ┬Ż(I_minus - I_plus)
-    return 0.5 * (I_minus - I_plus)
+    result = 0.5 * (I_minus - I_plus)
+
+    elapsed = time.perf_counter() - t_outer_start
+    if elapsed > _OUTER_SLOW_WARN_S and _should_sampled_debug("outer_integral_slow", every=50, initial=3):
+        logger.warning(
+            "Slow outer integral: %.1fs (r=[%.2f, %.2f], k_a=%.3f, k_b=%.3f, l_a=%d, l_b=%d)",
+            elapsed, r_m, r_max, k_a, k_b, l_a, l_b
+        )
+
+    return result
 
 
 
@@ -627,9 +789,9 @@ def levin_oscillatory_integral(
         
         # Evaluate functions at nodes (vectorized)
         # Use np.vectorize for scalar functions - creates efficient C loop
-        f_vals = np.vectorize(f_func)(r_nodes)
-        Phi_vals = np.vectorize(phi_func)(r_nodes)
-        Phi_prime_vals = np.vectorize(phi_prime_func)(r_nodes)
+        f_vals = _eval_callable_on_nodes(f_func, r_nodes)
+        Phi_vals = _eval_callable_on_nodes(phi_func, r_nodes)
+        Phi_prime_vals = _eval_callable_on_nodes(phi_prime_func, r_nodes)
         
         # Levin on this segment
         contrib = _levin_segment_complex(f_vals, r_nodes, Phi_vals, Phi_prime_vals)
@@ -802,14 +964,20 @@ def filon_oscillatory_integral(
     
     # Generate segments with constant phase increment
     dr_per_segment = delta_phi / abs(omega)
-    n_segments = max(1, int(np.ceil((r_end - r_start) / dr_per_segment)))
+    n_segments_raw = max(1, int(np.ceil((r_end - r_start) / dr_per_segment)))
+    n_segments = min(n_segments_raw, _FILON_MAX_SEGMENTS)
+    if n_segments_raw > _FILON_MAX_SEGMENTS and _should_sampled_debug("filon_segment_cap", every=200, initial=3):
+        logger.debug(
+            "Filon segments capped: raw=%d -> %d (omega=%.3f, r=[%.2f, %.2f], dphi=%.3f)",
+            n_segments_raw, n_segments, omega, r_start, r_end, delta_phi
+        )
     segment_bounds = np.linspace(r_start, r_end, n_segments + 1)
     
     contributions = []
     for i in range(n_segments):
         a, b = segment_bounds[i], segment_bounds[i + 1]
         r_nodes = np.linspace(a, b, n_nodes_per_segment)
-        f_vals = np.vectorize(f_func)(r_nodes)  # Vectorized
+        f_vals = _eval_callable_on_nodes(f_func, r_nodes)
         
         # omega is constant, phase_offset is the offset at r=0
         contrib = _filon_segment_complex(f_vals, r_nodes, omega, phase_offset)
@@ -891,20 +1059,22 @@ def compute_outer_integral_oscillatory(
     
     if linearity_test < filon_threshold:
         # Use Filon (constant frequency approximation)
-        logger.debug(
-            "Outer integral: using Filon (|╬Ž''|├Śh┬▓=%.2e < %.1f)",
-            linearity_test, filon_threshold
-        )
+        if _should_sampled_debug("outer_integral_filon", every=1000, initial=2):
+            logger.debug(
+                "Outer integral: using Filon (|╬Ž''|├Śh┬▓=%.2e < %.1f)",
+                linearity_test, filon_threshold
+            )
         return filon_oscillatory_integral(
             f_func, omega_mid, phi_func(r_m) - omega_mid * r_m,
             r_m, r_max, delta_phi
         )
     else:
         # Use Levin (handles nonlinear phase)
-        logger.debug(
-            "Outer integral: using Levin (|╬Ž''|├Śh┬▓=%.2e >= %.1f)",
-            linearity_test, filon_threshold
-        )
+        if _should_sampled_debug("outer_integral_levin", every=500, initial=2):
+            logger.debug(
+                "Outer integral: using Levin (|╬Ž''|├Śh┬▓=%.2e >= %.1f)",
+                linearity_test, filon_threshold
+            )
         # Estimate number of segments
         n_segments = max(1, int(np.ceil((r_max - r_m) / h)))
         return levin_oscillatory_integral(
@@ -918,7 +1088,8 @@ def compute_outer_integral_oscillatory(
 def check_phase_sampling(
     r: np.ndarray,
     k_total: float,
-    threshold: float = np.pi / 4
+    threshold: float = np.pi / 4,
+    eta_total: float = 0.0
 ) -> Tuple[float, bool, int]:
     """
     Check if grid adequately samples oscillations.
@@ -931,6 +1102,9 @@ def check_phase_sampling(
         Total wave number (k_i + k_f for product of waves).
     threshold : float
         Maximum allowed phase change per step (default ¤Ç/4).
+    eta_total : float
+        Total Sommerfeld parameter (╬Ě_i + ╬Ě_f) for ionic targets.
+        If nonzero, includes Coulomb phase component ╬Ě┬Ěln(r) in estimate.
         
     Returns
     -------
@@ -945,7 +1119,17 @@ def check_phase_sampling(
         return 0.0, True, -1
     
     dr = np.diff(r)
-    phase_per_step = k_total * dr
+    
+    # Phase per step for Coulomb waves:
+    #   ¤ć'(r) Ôëł k_total + eta_total / r
+    # so ╬ö¤ć Ôëł |¤ć'(r_mid)| ╬ör
+    if abs(eta_total) > 1e-10:
+        r_mid = np.maximum(0.5 * (r[:-1] + r[1:]), 1e-10)  # Midpoint of each interval
+        local_phase_derivative = k_total + eta_total / r_mid
+        phase_per_step = np.abs(local_phase_derivative * dr)
+    else:
+        phase_per_step = np.abs(k_total * dr)
+    
     max_phase = float(np.max(phase_per_step))
     
     # Find first problematic index
@@ -962,20 +1146,21 @@ def log_phase_diagnostic(
     k_i: float,
     k_f: float,
     l_i: int,
-    l_f: int
+    l_f: int,
+    eta_total: float = 0.0
 ) -> None:
     """
     Log diagnostic information about phase sampling.
     """
     k_total = k_i + k_f
-    max_phase, is_ok, prob_idx = check_phase_sampling(r, k_total)
+    max_phase, is_ok, prob_idx = check_phase_sampling(r, k_total, eta_total=eta_total)
     
     if not is_ok:
         r_problem = r[prob_idx] if prob_idx >= 0 else r[-1]
         logger.warning(
-            "Phase undersampling: l_i=%d, l_f=%d, k=%.2f+%.2f, "
+            "Phase undersampling: l_i=%d, l_f=%d, k=%.2f+%.2f, eta_total=%.2f, "
             "max_d¤ć=%.2f rad at r=%.0f bohr",
-            l_i, l_f, k_i, k_f, max_phase, r_problem
+            l_i, l_f, k_i, k_f, eta_total, max_phase, r_problem
         )
 
 
@@ -1166,6 +1351,7 @@ def integrate_with_phase_nodes(
     r_end: float,
     k_total: float,
     phase_increment: float = np.pi / 2,
+    eta_total: float = 0.0,
     use_clenshaw_curtis: bool = True,
     cc_nodes_per_interval: int = 5
 ) -> float:
@@ -1185,6 +1371,8 @@ def integrate_with_phase_nodes(
         Total wave number for phase calculation.
     phase_increment : float
         Phase change per sub-interval (default ¤Ç/2).
+    eta_total : float
+        Sum of Sommerfeld parameters (╬Ě_i + ╬Ě_f) for Coulomb phase correction.
     use_clenshaw_curtis : bool
         Use CC nodes within each interval (else trapezoid).
     cc_nodes_per_interval : int
@@ -1196,7 +1384,9 @@ def integrate_with_phase_nodes(
         Result of integration.
     """
     # Generate phase nodes
-    r_nodes = generate_phase_nodes(r_start, r_end, k_total, phase_increment)
+    r_nodes = generate_phase_nodes(
+        r_start, r_end, k_total, phase_increment, eta_total=eta_total
+    )
     
     if len(r_nodes) < 2:
         return 0.0
@@ -1651,10 +1841,10 @@ def _phase_adaptive_integrate(
             n_sub = int(np.ceil(dphi / max_phase_step))
             r_sub = np.linspace(r[i], r[i+1], n_sub + 1)
             
-            # Linear interpolation for all arrays
-            f_sub = np.interp(r_sub, r, f_vals)
-            chi_i_sub = np.interp(r_sub, r, chi_i)
-            chi_f_sub = np.interp(r_sub, r, chi_f)
+            # Grid-aware interpolation for all arrays (log-r on exponential grids).
+            f_sub = _interp_on_radial_grid(r_sub, r, f_vals)
+            chi_i_sub = _interp_on_radial_grid(r_sub, r, chi_i)
+            chi_f_sub = _interp_on_radial_grid(r_sub, r, chi_f)
             
             integrand_sub = f_sub * chi_i_sub * chi_f_sub
             result += np.trapz(integrand_sub, r_sub)
@@ -1681,7 +1871,8 @@ def oscillatory_radial_integral_1d(
     l_i: int = 0,
     l_f: int = 0,
     use_analytical_tail: bool = True,
-    use_filon: bool = True
+    use_filon: bool = True,
+    eta_total: float = 0.0
 ) -> float:
     """
     Compute oscillatory radial integral with proper handling of high-frequency content.
@@ -1714,6 +1905,8 @@ def oscillatory_radial_integral_1d(
         Whether to add analytical tail contribution.
     use_filon : bool
         Whether to use Filon quadrature (else phase-adaptive).
+    eta_total : float
+        Sum of Sommerfeld parameters (╬Ě_i + ╬Ě_f) for Coulomb phase correction.
         
     Returns
     -------
@@ -1737,7 +1930,7 @@ def oscillatory_radial_integral_1d(
     
     # Check phase sampling
     k_total = k_i + k_f
-    max_phase, is_ok, _ = check_phase_sampling(r_num, k_total)
+    max_phase, is_ok, _ = check_phase_sampling(r_num, k_total, eta_total=eta_total)
     
     if is_ok or not use_filon:
         # Standard weighted integration is OK
@@ -1889,7 +2082,9 @@ def oscillatory_kernel_integral_2d(
     elif method == "adaptive":
         # Phase-aware integration for outer integral
         k_total = k_i + k_f
-        max_phase, is_ok, prob_idx = check_phase_sampling(r1_lim, k_total)
+        max_phase, is_ok, prob_idx = check_phase_sampling(
+            r1_lim, k_total, eta_total=eta_total
+        )
         
         if is_ok:
             # Standard is OK - fully vectorized with weights
@@ -1922,7 +2117,11 @@ def oscillatory_kernel_integral_2d(
         # ==========================================================================
         
         dr = np.diff(r1_lim)
-        phase_per_step = k_total * dr
+        if abs(eta_total) > 1e-10:
+            r_mid = np.maximum(0.5 * (r1_lim[:-1] + r1_lim[1:]), 1e-10)
+            phase_per_step = np.abs((k_total + eta_total / r_mid) * dr)
+        else:
+            phase_per_step = np.abs(k_total * dr)
         max_phase_threshold = np.pi / 4
         
         # Count undersampled regions for logging
@@ -1974,7 +2173,9 @@ def oscillatory_kernel_integral_2d(
         else:
             # Generate phase nodes
             r_start, r_end = r1_lim[0], r1_lim[-1]
-            phase_nodes = generate_phase_nodes(r_start, r_end, k_total, PHASE_INCREMENT)
+            phase_nodes = generate_phase_nodes(
+                r_start, r_end, k_total, PHASE_INCREMENT, eta_total=eta_total
+            )
             n_intervals = len(phase_nodes) - 1
             
             if n_intervals < 1:
@@ -2010,8 +2211,8 @@ def oscillatory_kernel_integral_2d(
                     all_r_flat = all_r.ravel()
                     
                     # Interpolate rho1 and int_r2
-                    rho1_interp = np.interp(all_r_flat, r1_lim, rho1_lim)
-                    int_r2_interp = np.interp(all_r_flat, r1_lim, int_r2)
+                    rho1_interp = _interp_on_radial_grid(all_r_flat, r1_lim, rho1_lim)
+                    int_r2_interp = _interp_on_radial_grid(all_r_flat, r1_lim, int_r2)
                     
                     # Compute integrand
                     integrand = rho1_interp * int_r2_interp
@@ -2055,7 +2256,9 @@ def oscillatory_kernel_integral_2d(
         else:
             # Generate phase nodes
             r_start, r_end = r1_lim[0], r1_lim[-1]
-            phase_nodes = generate_phase_nodes(r_start, r_end, k_total, PHASE_INCREMENT)
+            phase_nodes = generate_phase_nodes(
+                r_start, r_end, k_total, PHASE_INCREMENT, eta_total=eta_total
+            )
             n_intervals = len(phase_nodes) - 1
             
             if n_intervals < 1:
@@ -2088,7 +2291,7 @@ def oscillatory_kernel_integral_2d(
                     # For each r1, compute Ôłź K(r1, r2) ├Ś rho2(r2) dr2 using CC
                     # 
                     # Interpolate kernel and rho2 at CC nodes
-                    rho2_interp = np.interp(all_r_flat, r2_lim, rho2_lim)
+                    rho2_interp = _interp_on_radial_grid(all_r_flat, r2_lim, rho2_lim)
                     rho2_cc = rho2_interp.reshape(n_valid, n_nodes)
                     
                     # For kernel, we need K(r1, r2_cc) for each r1
@@ -2132,8 +2335,8 @@ def oscillatory_kernel_integral_2d(
                     # OUTER INTEGRAL with CC
                     # =========================================================
                     # Interpolate rho1 and int_r2_cc at CC nodes
-                    rho1_interp = np.interp(all_r_flat, r1_lim, rho1_lim)
-                    int_r2_outer = np.interp(all_r_flat, r1_lim, int_r2_cc)
+                    rho1_interp = _interp_on_radial_grid(all_r_flat, r1_lim, rho1_lim)
+                    int_r2_outer = _interp_on_radial_grid(all_r_flat, r1_lim, int_r2_cc)
                     
                     # Compute outer integrand
                     outer_integrand = rho1_interp * int_r2_outer
